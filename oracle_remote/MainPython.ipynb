{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cleanDataset\n",
      "Imported clean dataset!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:85% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gensim.downloader as api\n",
    "import numpy as np\n",
    "import re\n",
    "import csv\n",
    "import pandas as pd\n",
    "import pprint\n",
    "import string\n",
    "import nltk\n",
    "import sys\n",
    "import spacy\n",
    "import os\n",
    "import pickle\n",
    "import random     \n",
    "import json\n",
    "\n",
    "from IPython.display import HTML\n",
    "from nltk.corpus import wordnet \n",
    "from sklearn.manifold import TSNE\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "pathToDatasets = '../datasets/'\n",
    "pathToDataScripts = '../datasets/scripts/'\n",
    "filePath = '../datasets/GoogleNews-vectors-negative300.bin'\n",
    "# modelBeingUsed = \"glove-wiki-gigaword-300\"\n",
    "# modelBeingUsed = \"glove-wiki-gigaword-100\"\n",
    "# modelBeingUsed = \"glove-twitter-50\"\n",
    "modelBeingUsed = \"glove-twitter-100\"\n",
    "# modelBeingUsed = \"glove-twitter-200\"\n",
    "\n",
    "# DATASET_FILE_NAME = \"shuffled_twitter_corpus_output\"\n",
    "# DATASET_FILE_NAME = \"airplane_tweets\"\n",
    "DATASET_FILE_NAME = \"cleanedTweets\"\n",
    "\n",
    "\n",
    "\n",
    "sys.path.insert(0, pathToDataScripts)\n",
    "from cleanDataset import tokenize_words, dataClean\n",
    "\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:85% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading binaries and models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Should I reload the model?\n",
      "Yes\n",
      "loading glove-twitter-100!\n",
      "loading via pickle!\n",
      "Model Loaded!\n",
      "{'so', 'slightly', 'totally', 'very', 'extremely', 'completely', 'especially', 'incredibly', 'definitely', 'absolutely', 'really'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "print(\"Should I reload the model?\")\n",
    "tstString = input()\n",
    "if(\"no\" in tstString.lower() or \"n\" in tstString.lower()):\n",
    "    print(\" didnt reload model! \")\n",
    "else:\n",
    "    print(\"loading {0}!\".format(modelBeingUsed));\n",
    "    fileName = \"{}.pickle\".format(modelBeingUsed)\n",
    "    if(os.path.exists(pathToDatasets+fileName)):\n",
    "        print(\"loading via pickle!\")\n",
    "        pickle_in = open(pathToDatasets+fileName, \"rb\")\n",
    "        word_vectors = pickle.load(pickle_in);\n",
    "    else:\n",
    "        print(\"Pickle didn't exist, therefore loading model!\")\n",
    "        word_vectors = api.load(\"{0}\".format(modelBeingUsed))\n",
    "        print(\"-- Saving to pickle file for next time! --\")\n",
    "        pickle_out = open(pathToDatasets+fileName,\"wb\")\n",
    "        pickle.dump(word_vectors, pickle_out)\n",
    "        pickle_out.close()\n",
    "        \n",
    "    nltk.download('vader_lexicon')\n",
    "    nltk.download('punkt')\n",
    "    nltk.download('averaged_perceptron_tagger')\n",
    "    nltk.download('wordnet')\n",
    "    print(\"Model Loaded!\")\n",
    "\n",
    "setOfBoosterWords = set()\n",
    "with open(pathToDatasets + \"BoosterWordList.txt\") as bwf:\n",
    "    pd_booster = pd.read_csv(bwf, sep='\\t');\n",
    "    listOfBoosterWords = (pd_booster.iloc[:,0]).tolist() \n",
    "    setOfBoosterWords = set(listOfBoosterWords[:])\n",
    "print(setOfBoosterWords)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global Variables and Global Objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "senty = SentimentIntensityAnalyzer()\n",
    "vocabulary = word_vectors.vocab;\n",
    "\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "\n",
    "## OPTIONS_HELPER\n",
    "\n",
    "'''\n",
    "Good sets -> {100+20}\n",
    "'''\n",
    "TWEET_START = 5000\n",
    "NUM_OF_TWEETS = 3\n",
    "\n",
    "# VERBOSE_PRINTING = True\n",
    "VERBOSE_PRINTING = False\n",
    "\n",
    "# USE_SPACY = False\n",
    "USE_SPACY = True\n",
    "\n",
    "COLOR_PRINTING = True\n",
    "# COLOR_PRINTING = False\n",
    "\n",
    "# PRINT_NEUTRAL = True\n",
    "PRINT_NEUTRAL = False\n",
    "\n",
    "# PRINT_ALL_STRINGS = True\n",
    "PRINT_ALL_STRINGS = False\n",
    "\n",
    "HANDLE_NEGATIONS = True\n",
    "# HANDLE_NEGATIONS = False\n",
    "\n",
    "# HANDLE_BOOSTERS = True\n",
    "HANDLE_BOOSTERS = False\n",
    "\n",
    "KEEP_SENSE = True\n",
    "# KEEP_SENSE = False\n",
    "\n",
    "SHOW_ALTS = 15\n",
    "\n",
    "NUMBER_OF_NEAR = 8\n",
    "MAX_SENTIMENT_HEAVY_WORDS = 10\n",
    "MAX_REPLACEMENTS = 5 # increasing this has a x(NUM_OF_SENTIMENT_WORDS) effect.\n",
    "\n",
    "PROB_ADDING_BOOSTER = 0.30 # Keep between 0 and 1, if you want all possible, use 1. If you want none, use 0\n",
    "\n",
    "\n",
    "punctuation = r\"\\\"#$%&'+-/;<=>@[\\]*^_`{|}~.\"\n",
    "LIST_OF_NEGATIONS = [\"not\", \"no\"]\n",
    "BANNED_WORDS = [\"fuck\",\"bitch\", \"motherfucker\"]\n",
    "\n",
    "LIST_OF_EVAL_OBJECTS = []\n",
    "LIST_OF_TAGS_TO_IGNORE = [\"IN\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class for Sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imported class file!\n"
     ]
    }
   ],
   "source": [
    "from SentenceClass import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EvaluationObj:\n",
    "    def __init__(self, main, best, worst, mainSent, bestSent, worstSent, uniqueID):\n",
    "        self.main  = main\n",
    "        self.best = best\n",
    "        self.worst = worst\n",
    "        self.mainSent = mainSent\n",
    "        self.bestSent = bestSent\n",
    "        self.worstSent = worstSent\n",
    "        self.uniqueID = uniqueID\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printStrings(sentenceObj, uniqueID = -1):\n",
    "    \n",
    "    \n",
    "    numberOfPrints = 0\n",
    "    newStrings = generateHTMLObjectsFromSentence(sentenceObj)\n",
    "    mainSentiment = sentenceObj.ogSentiment;\n",
    "    listOfSentencesWithSentiments = []\n",
    "    bestSentiment = -sys.maxsize - 1\n",
    "    worstSentiment = sys.maxsize\n",
    "    \n",
    "    isPositiveSentence = True if(mainSentiment >= 0) else False\n",
    "    \n",
    "    bestSentimentString = \"\";\n",
    "    worstSentimentString = \"\";\n",
    "    \n",
    "    if(uniqueID != -1):\n",
    "        global LIST_OF_EVAL_OBJECTS\n",
    "    \n",
    "    numberOfSentimentHeavyWordsYet = 0;\n",
    "    for ind, tSentence in enumerate(newStrings):\n",
    "            alteredTweet = tSentence.getSentence()\n",
    "            htmlText = tSentence.getHTML()\n",
    "            numberOfPrints+=1;\n",
    "            \n",
    "            \n",
    "            sentimentOfNewString = tSentence.getSentiment() #senty.polarity_scores(alteredTweet)['compound']\n",
    "            \n",
    "            thisStringPositive = True if(sentimentOfNewString >= 0) else False\n",
    "            if(thisStringPositive != isPositiveSentence and KEEP_SENSE):\n",
    "                continue\n",
    "            \n",
    "            newObj = SentenceWithSentiment(alteredTweet, sentimentOfNewString, htmlText)\n",
    "            listOfSentencesWithSentiments.append(newObj)\n",
    "            \n",
    "            if(sentimentOfNewString >= bestSentiment):\n",
    "                bestSentiment = sentimentOfNewString\n",
    "                bestSentimentString = htmlText;\n",
    "            elif(sentimentOfNewString < worstSentiment):\n",
    "                worstSentiment = sentimentOfNewString\n",
    "                worstSentimentString = htmlText;\n",
    "            \n",
    "            \n",
    "#             if(numberOfPrints > SHOW_ALTS): break\n",
    "            if(numberOfPrints > SHOW_ALTS or PRINT_ALL_STRINGS == False): continue\n",
    "            if(sentimentOfNewString == mainSentiment or sentimentOfNewString == 0.0):\n",
    "                if(PRINT_ALL_STRINGS and numberOfPrints <= SHOW_ALTS): displayText(\"{0}: {1}\".format(htmlText,sentimentOfNewString),'black')\n",
    "            elif(sentimentOfNewString > mainSentiment):\n",
    "                if(PRINT_ALL_STRINGS and numberOfPrints <= SHOW_ALTS): displayText(\"{0}: {1}\".format(htmlText,sentimentOfNewString),'green')\n",
    "            elif(sentimentOfNewString < mainSentiment and sentimentOfNewString != 0.0):\n",
    "                if(PRINT_ALL_STRINGS and numberOfPrints <= SHOW_ALTS): displayText(\"{0}: {1}\".format(htmlText,sentimentOfNewString),'red')\n",
    "                \n",
    "#     if(numberOfPrints > SHOW_ALTS): print(\"--- More options (total: {0}) possible, but not printed ---\".format(numberOfPrints));\n",
    "    print(\"--- More options (total: {0}) possible ---\".format(numberOfPrints));\n",
    "    displayText(\"Actual Sentence: {0} :{1}\".format(sentenceObj.ogSentence, sentenceObj.ogSentiment))\n",
    "    if (worstSentimentString != \"\"): displayText(\"Worst Sentence: {0} : {1}\".format(worstSentimentString, worstSentiment), color='red')\n",
    "    if (bestSentimentString != \"\"): displayText(\"Best Sentence: {0} : {1}\".format(bestSentimentString, bestSentiment), color='green') \n",
    "        \n",
    " \n",
    "        \n",
    "    if(worstSentimentString != \"\" and bestSentimentString != \"\" and uniqueID != -1):\n",
    "#             file_writeAll.write(\"Inp({3}): {0}\\nB({3}): {1}\\nW({3}): {2}\\nInSent:{4}, BSent:{5}, WSent:{6}\\n\\n\".format(sentenceObj.ogSentence, bestSentimentString, worstSentimentString, uniqueID, sentenceObj.ogSentiment, bestSentiment, worstSentiment));\n",
    "            thisEvalObj = EvaluationObj(sentenceObj.ogSentence, bestSentimentString, worstSentimentString, sentenceObj.ogSentiment, bestSentiment, worstSentiment, uniqueID);\n",
    "            LIST_OF_EVAL_OBJECTS.append(thisEvalObj)\n",
    "            \n",
    "#             print(\"Wrote something!\")\n",
    "            \n",
    "\n",
    "    sentenceObj.addFinalSentences(listOfSentencesWithSentiments)\n",
    "    return listOfSentencesWithSentiments, sentenceObj;\n",
    "\n",
    "def cstr(s, color='black', italics=False):\n",
    "    if(COLOR_PRINTING):\n",
    "        if(italics):\n",
    "            return cstr(\"{0}\".format(s), color);\n",
    "        return \"<text style=color:{}>{}</text>\".format(color, s)\n",
    "    else:\n",
    "        return \"{}\".format(s)\n",
    "\n",
    "def displayText(text, color='black'):\n",
    "    if(COLOR_PRINTING):\n",
    "        display(HTML(cstr(text, color)));\n",
    "        return\n",
    "    print(\"{}\".format(text));\n",
    "    \n",
    "    \n",
    "def cleanAndTokenizeText(text):\n",
    "#     text = text.lower();\n",
    "    newString = \"\"\n",
    "    for char in text:\n",
    "        if char not in punctuation:\n",
    "            newString += char\n",
    "#     text = word_tokenize(newString)\n",
    "    text = newString.split()\n",
    "    return text;\n",
    "\n",
    "def getPOSTags(tweet):\n",
    "    if(USE_SPACY == False):\n",
    "        tags = nltk.pos_tag(tweet)\n",
    "        return tags;    \n",
    "    tweet = ' '.join(tweet)\n",
    "    doc = nlp(tweet)\n",
    "    tags = [(token.text, token.tag_) for token in doc] # since the format expected is [text,tag]\n",
    "    return tags\n",
    "\n",
    "def getAntonymsAndSynonymsOfWords(word):\n",
    "    loweredWord = word.lower()\n",
    "    if(word not in vocabulary):\n",
    "        if(loweredWord in vocabulary):\n",
    "            word = loweredWord;\n",
    "        else:\n",
    "            return [], []\n",
    "    setOfAntonyms = set()\n",
    "    setOfSynonyms = set()\n",
    "    for syn in wordnet.synsets(word):\n",
    "        for l in syn.lemmas():\n",
    "            setOfSynonyms.add(l.name().lower())\n",
    "            anton = l.antonyms()\n",
    "            if(anton!=[]):\n",
    "                setOfAntonyms.add(anton[0].name().lower())\n",
    "    if(len(setOfAntonyms) == 0):\n",
    "        if(VERBOSE_PRINTING): print(\"No antonyms found for word {0}\".format(word))\n",
    "    if(len(setOfSynonyms) == 0):\n",
    "        if(VERBOSE_PRINTING): print(\"No synonyms found for word {0}\".format(word))\n",
    "            \n",
    "    if(KEEP_SENSE):\n",
    "        return [], list(setOfSynonyms)\n",
    "    else:    \n",
    "        return list(setOfAntonyms), list(setOfSynonyms)\n",
    "\n",
    "\n",
    "def returnReplacementsForWord(word):\n",
    "    loweredWord = word.lower()\n",
    "    if(word not in vocabulary):\n",
    "        if(loweredWord in vocabulary):\n",
    "            word = loweredWord;\n",
    "        else:\n",
    "            print(\" --- |{0}| not in vocabulary ---\".format(word))\n",
    "            return []\n",
    "       \n",
    "    possibleReplacements = [word[0] for word in word_vectors.most_similar(word,topn=NUMBER_OF_NEAR)]\n",
    "    \n",
    "    if(VERBOSE_PRINTING): print(\"Some word_embedding replacements for {} are : {}\".format(word,possibleReplacements))\n",
    "    if(possibleReplacements == []):\n",
    "        print(\" --- No replacements for word {0} ---\".format(word))\n",
    "    antonyms,synonyms = getAntonymsAndSynonymsOfWords(word)\n",
    "    \n",
    "    if(antonyms != []):\n",
    "        possibleReplacements.extend(antonyms)\n",
    "        if(VERBOSE_PRINTING): print(\"Some antonyms for word {0} are {1}\".format(word, antonyms[:3]))\n",
    "    if(synonyms != []):\n",
    "        possibleReplacements.extend(synonyms)\n",
    "        if(VERBOSE_PRINTING): print(\"Some synonyms for word {0} are {1}\".format(word, synonyms[:3]))\n",
    "    return possibleReplacements\n",
    "    \n",
    "def posApprovedReplacements(alternativeWords, userTokens, indexOfToken):\n",
    "    if(alternativeWords == []):\n",
    "        return []\n",
    "    tempTokens = userTokens[:]\n",
    "    POSTokens = getPOSTags(tempTokens)\n",
    "    validWords = []\n",
    "    \n",
    "    mainTag = POSTokens[indexOfToken][1]\n",
    "    mainWord = userTokens[indexOfToken]\n",
    "    \n",
    "    for ind,word in enumerate(alternativeWords):\n",
    "        if(\"_\" in word): continue\n",
    "        tempTokens[indexOfToken] = word\n",
    "        posTags = getPOSTags(tempTokens)\n",
    "        newTag = (posTags[indexOfToken])[1]\n",
    "#         if(VERBOSE_PRINTING): print(\"mainTag({0}):{1} | alternative({2}):{3}\".format(mainWord, mainTag, word, newTag))\n",
    "        if(str(newTag) == str(mainTag)):\n",
    "            validWords.append(word)\n",
    "    if(validWords == [] and VERBOSE_PRINTING):\n",
    "        print(\"No POS words found for word {} with tag {}\".format(mainWord, mainTag));\n",
    "    return validWords\n",
    "    \n",
    "def editBoosterWords(sentenceObj):\n",
    "    if(HANDLE_BOOSTERS == False): return sentenceObj;\n",
    "    ogSentence = sentenceObj.ogSentence;\n",
    "    \n",
    "    tokenizedSentence = sentenceObj.getSentenceTokens()\n",
    "    \n",
    "    '''\n",
    "        Use the indexToAlternatives dictionary to add words into the sentences, as part of the chunking process \n",
    "        The problem with this instinctively is how do you have an index \"between\" two variables \n",
    "        You can't really. You'd have to have a flag that says which places it's meant to go \n",
    "        You could do it _before_ the sentiment heavy word (this makes more natural sense) or \n",
    "            you could do it _after_ the word before the sentiment heavy word (this could have its own advantages) \n",
    "    '''\n",
    "    \n",
    "    posTagsForWords = getPOSTags(tokenizedSentence)  \n",
    "    if(VERBOSE_PRINTING): \n",
    "        print(\"Length of posTagsForWords: {0}\".format(len(posTagsForWords)))\n",
    "        if(len(posTagsForWords) > len(tokenizedSentence)):\n",
    "            print(\"Number of tags more than tokenized sentence!\");\n",
    "            \n",
    "    for ind,(word,tag) in enumerate(posTagsForWords):\n",
    "        tmp = ' '.join(tokenizedSentence[ind:ind+2]) # Temp is used to obtain some context around the word.\n",
    "        if(word in setOfBoosterWords):\n",
    "            if(senty.polarity_scores(tmp)['compound'] != 0):\n",
    "                sentenceObj.addAlternativesByIndex(ind, [\"IGNORE_FLAG\", word]);\n",
    "                continue\n",
    "                \n",
    "        elif(senty.polarity_scores(word)['compound'] != 0 and (\"RB\" in tag or \"JJ\" in tag or \"VB\" in tag)): \n",
    "            if(ind - 1 < 0 or ind-1 >= len(tokenizedSentence)):\n",
    "                if(VERBOSE_PRINTING): print(\" Out of Bounds: index : {0}, lenOfSentence: {1}\".format(ind, len(tokenizedSentence)))\n",
    "                continue;\n",
    "                \n",
    "            if(tag in LIST_OF_TAGS_TO_IGNORE): continue\n",
    "            if(tokenizedSentence[ind-1] in LIST_OF_NEGATIONS):\n",
    "                newInd = ind-2 if(ind-2 >=0 ) else 0\n",
    "            else:\n",
    "                newInd = ind-1 if ((ind-1) >= 0 ) else 0\n",
    "            randNum = random.uniform(0,1)\n",
    "            if(randNum < PROB_ADDING_BOOSTER):\n",
    "                sentenceObj.addAlternativesByIndex(newInd, [\"BOOSTER_FLAG\"]);\n",
    "                if(VERBOSE_PRINTING): print(\"Placed booster at {0} from word {1} on word {2} with tag {3}\".format(newInd, word,tokenizedSentence[newInd], tag ))\n",
    "    \n",
    "    return sentenceObj\n",
    "\n",
    "\n",
    "def generateHTMLObjectsFromSentence(sentenceObj):\n",
    "    \n",
    "    allSentences = sentenceObj.getFinalSentences()\n",
    "    indexToAlts = sentenceObj.indexToSetOfWords;\n",
    "    indexToChange = list(indexToAlts.keys());\n",
    "    \n",
    "    numberOfNegationsRemoved = 0;\n",
    "    listOfSentenceObjs = []\n",
    "    for t_sentenceObj in allSentences:\n",
    "        sentence = t_sentenceObj.getSentence()\n",
    "        copySentence = t_sentenceObj.getSentenceTokens()\n",
    "        subIndex = 0\n",
    "        addIndex = 0\n",
    "        for index in sorted(indexToChange):\n",
    "            if(index >= len(copySentence) or index<0 ):\n",
    "#                 print(\"---Out of Bounds found :{0}/{1}\".format(index, len(copySentence)))\n",
    "                continue\n",
    "            if(index >= len(copySentence)):  \n",
    "                copySentence[len(copySentence) - index] = cstr(\"{0}\".format(copySentence[len(copySentence) - index]), \"blue\", italics=True);\n",
    "                continue\n",
    "            else:\n",
    "                if(\"IGNORE_FLAG\" in indexToAlts[index]):\n",
    "                    if(copySentence[index] in LIST_OF_NEGATIONS):\n",
    "                        continue; ## Word that _should_ be removed is present.\n",
    "                    else:\n",
    "                        subIndex +=1\n",
    "                        numberOfNegationsRemoved +=1;\n",
    "                        continue;\n",
    "                elif(\"BOOSTER_FLAG\" in indexToAlts[index]):\n",
    "                    addIndex +=1;\n",
    "\n",
    "            newIndex = index + addIndex - subIndex;\n",
    "            if(newIndex >= len(copySentence)):\n",
    "                newIndex = len(copySentence)-1\n",
    "            elif(newIndex < 0):\n",
    "                newIndex = 0;\n",
    "            \n",
    "            \n",
    "            copySentence[newIndex] = cstr(\"{0}\".format(copySentence[newIndex]), \"blue\", italics=True);\n",
    "        listOfSentenceObjs.append(SentenceWithHTML(sentence, ' '.join(copySentence), t_sentenceObj.getSentiment()));\n",
    "    \n",
    "    if(VERBOSE_PRINTING): print(\"Number of negations removed : {0}\".format(numberOfNegationsRemoved))\n",
    "    return listOfSentenceObjs\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence Chunking and Appending"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def helper_combine(mainList, myList):\n",
    "    '''\n",
    "    helper function for CombineSentenceChunks\n",
    "    '''\n",
    "    newList = []\n",
    "    for val in myList:\n",
    "        for mainVal in mainList:\n",
    "#             if(VERBOSE_PRINTING): print(\"Combining {0} with {1}\".format(' '.join(val), ' '.join(mainVal)));\n",
    "            newList.append(val + mainVal);\n",
    "    return newList;\n",
    "\n",
    "def combineSentenceChunks(wholeSentence, dictOfChunks):\n",
    "    '''\n",
    "        Uses the helper_combine function to generate all possible combinatios and permuations of the chunks\n",
    "        and any alternatives.\n",
    "        \n",
    "        The logic is to use the end of the sentence, apply each possible chunk from the previous key's chunks\n",
    "        to every possible chunk of this key's.\n",
    "        \n",
    "        The helper function is used to allow us to reuse the list of alreadyGeneratedChunks and constantly\n",
    "        append to them.\n",
    "        \n",
    "        To understand the logic better, take a look at this gist:\n",
    "        https://gist.github.com/sunnyMiglani/cf85407a9e6928237b1436cc2bc95fa4\n",
    "        \n",
    "    '''\n",
    "    reversedKeys = sorted(dictOfChunks.keys(), reverse=True)\n",
    "    mainArr = dictOfChunks[reversedKeys[0]]\n",
    "    for ind in range(1, len(reversedKeys)):\n",
    "        key = reversedKeys[ind];\n",
    "        mainArr = helper_combine(mainArr, dictOfChunks[key]);\n",
    "        \n",
    "    return mainArr;\n",
    "        \n",
    "def generateSentenceChunks(wholeSentence, keyToChange, nextKey, listOfMyAlternatives):\n",
    "    '''\n",
    "        Generates sentence chunks by iterating through the list of alternatives\n",
    "        Chunking the sentence to start from current key to next key.\n",
    "        This means that the sentence always goes from key 'x' to key 'y'\n",
    "        \n",
    "        Example:\n",
    "        \"I really <hate> hot chocolate, but I <prefer> hot coffee\"\n",
    "        Calling generateSentenceChunks will create an example sentence:\n",
    "            - \"<altWordForHate> hot chocolate , but I \"\n",
    "        \n",
    "        Remember to append the first stretch of the string to the first key's chunk for proper use!\n",
    "    '''\n",
    "    \n",
    "    newList = list(listOfMyAlternatives)\n",
    "    if(keyToChange < len(wholeSentence) and keyToChange >= 0):\n",
    "        newList.append(wholeSentence[keyToChange]);\n",
    "    else:\n",
    "        return []\n",
    "    generatedSentences = []\n",
    "    \n",
    "        \n",
    "    for myAlt in newList:\n",
    "        if(\"IGNORE_FLAG\" == myAlt):\n",
    "            newSentence = wholeSentence[keyToChange+1:nextKey];\n",
    "            generatedSentences.append(newSentence)\n",
    "            continue\n",
    "        elif(\"BOOSTER_FLAG\" == myAlt): \n",
    "            for booster in setOfBoosterWords:\n",
    "                newSentence = wholeSentence[:]\n",
    "                newSentence.insert(keyToChange+1, booster)\n",
    "                generatedSentences.append(newSentence[keyToChange:nextKey+1]);\n",
    "        else:\n",
    "            newSentence = wholeSentence[:]\n",
    "            newSentence[keyToChange] = myAlt\n",
    "            generatedSentences.append(newSentence[keyToChange:nextKey]);\n",
    "        \n",
    "    return generatedSentences\n",
    "    \n",
    "def returnCombinationsOfStrings(sentenceObj):\n",
    "    \n",
    "    indexToWordDict = sentenceObj.indexToSetOfWords;\n",
    "    originalSentence = sentenceObj.ogSentence;\n",
    "    tokenizedSentence = sentenceObj.getSentenceTokens() \n",
    "    reversedKeys = sorted(indexToWordDict.keys(), reverse=True)\n",
    "    dictAlternatives  = {}\n",
    "\n",
    "    sortedKeys = sorted(indexToWordDict.keys())\n",
    "    sentenceChunks = {}\n",
    "    htmlChunks = {}\n",
    "    \n",
    "    for ind in range(0,len(sortedKeys)):\n",
    "        key = sortedKeys[ind]\n",
    "        nextKey = sortedKeys[ind+1] if ind+1 < len(sortedKeys) else len(tokenizedSentence)\n",
    "        sentenceChunks[key] = generateSentenceChunks(tokenizedSentence, key, nextKey, indexToWordDict[key])\n",
    "\n",
    "    if(sortedKeys[0] != 0):\n",
    "        newList = []\n",
    "        for thislist in sentenceChunks[sortedKeys[0]]:\n",
    "            newList.append(tokenizedSentence[0:sortedKeys[0]] + thislist)\n",
    "        sentenceChunks[sortedKeys[0]] = newList;\n",
    "        \n",
    "    finalOptions = combineSentenceChunks(tokenizedSentence, sentenceChunks)\n",
    "    \n",
    "    finalSentences = []\n",
    "    for val in finalOptions:\n",
    "        sentence = ' '.join(val)\n",
    "        sentiment = senty.polarity_scores(sentence)['compound']\n",
    "        newSentenceObj = SentenceWithSentiment(sentence,sentiment)\n",
    "        newSentenceObj.setSentenceTokens(val)\n",
    "        finalSentences.append(newSentenceObj)\n",
    "    \n",
    "    sentenceObj.addFinalSentences(finalSentences)   \n",
    "    return sentenceObj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trimSuggestions(sentenceObj, mainWord, listOfAlternatives):\n",
    "    mainSentiment = senty.polarity_scores(mainWord)['compound']\n",
    "    isPositive = True if(mainSentiment > 0) else False\n",
    "    finalWords = []\n",
    "    for word in listOfAlternatives:\n",
    "        if(word) in BANNED_WORDS: continue\n",
    "        senty_suggestion = senty.polarity_scores(word)['compound'];\n",
    "        isPositive_suggestion = True if senty_suggestion > 0 else False\n",
    "        if(KEEP_SENSE and isPositive_suggestion != isPositive): continue\n",
    "        if(senty_suggestion == 0):\n",
    "            continue\n",
    "        else:\n",
    "            finalWords.append(word)\n",
    "    if(len(finalWords) == 1):\n",
    "        if(finalWords[0] == mainWord): return []\n",
    "    return finalWords\n",
    "\n",
    "def getAlternativeWords(sentenceObj):\n",
    "    mainSentence = sentenceObj.ogSentence;\n",
    "    mainSentiment = sentenceObj.ogSentiment;\n",
    "\n",
    "    sentenceTokens = cleanAndTokenizeText(mainSentence)\n",
    "    sentenceObj.setSentenceTokens(sentenceTokens)\n",
    "    posTagsOfSentence = getPOSTags(sentenceTokens);\n",
    "    \n",
    "    \n",
    "    numberOfSentimentHeavyWords = 0;\n",
    "    for ind, word in enumerate(sentenceTokens):\n",
    "        alternativeSentenceWithHTML = []\n",
    "        copyOfTokens = sentenceTokens[:]\n",
    "        replacements = []\n",
    "        replacements = sentenceObj.checkIfWordExists(word)\n",
    "        if(replacements != []):\n",
    "            print(\"FOUND REPEATED WORD {0}\".format(word))\n",
    "            sentenceObj.addAlternativesByIndex(ind, replacements)\n",
    "            continue;\n",
    "        \n",
    "        if(HANDLE_NEGATIONS):\n",
    "            if(word in LIST_OF_NEGATIONS):\n",
    "                \n",
    "                '''\n",
    "                    If the word is in the list of negations\n",
    "                    We add a [IGNORE_FLAG] tag to the alternatives\n",
    "                    Which will be grabbed in the future by the sentence generation algorithm. (chunking)\n",
    "                '''\n",
    "                \n",
    "                tmp = ' '.join(sentenceTokens[ind:ind+3])\n",
    "                if(senty.polarity_scores(tmp)['compound'] != 0):\n",
    "                    sentenceObj.addAlternativesByIndex(ind, [\"IGNORE_FLAG\", word]);\n",
    "                    continue\n",
    "\n",
    "        score = senty.polarity_scores(word)['compound']\n",
    "        if(score != 0.0):\n",
    "            if(posTagsOfSentence[ind][0] == word and posTagsOfSentence[ind][1] in LIST_OF_TAGS_TO_IGNORE): continue\n",
    "            if(numberOfSentimentHeavyWords < MAX_SENTIMENT_HEAVY_WORDS):\n",
    "                numberOfSentimentHeavyWords+=1;\n",
    "            else:\n",
    "                print(\"Sentiment heavy words:  {0}/{1}\".format(numberOfSentimentHeavyWords, MAX_SENTIMENT_HEAVY_WORDS))\n",
    "                return sentenceObj\n",
    "            \n",
    "            tempReplacements = returnReplacementsForWord(word) # get embedding based relations\n",
    "            \n",
    "            if(VERBOSE_PRINTING): print(\"Some early replacements for word {0} are {1}\".format(word, tempReplacements[0:8]))\n",
    "            if(tempReplacements == []):\n",
    "                print(\"No replacements found at all for word {0}\".format(word))\n",
    "                continue\n",
    "            \n",
    "            replacements = posApprovedReplacements(tempReplacements[:], copyOfTokens[:], ind)\n",
    "            finalReplacements = trimSuggestions(sentenceObj,word, replacements)\n",
    "            \n",
    "            if(finalReplacements == []):\n",
    "                if(VERBOSE_PRINTING): print(\" -- No POS approved words after trimming! -- for word {0}\\n some non-trimmed:{1}\".format(word, tempReplacements[:4]))\n",
    "                continue\n",
    "            \n",
    "            # Trimming until max replacements to ensure efficiency\n",
    "            finalReplacements = finalReplacements[:MAX_REPLACEMENTS]\n",
    "            sentenceObj.addAlternativesByIndex(ind, finalReplacements)\n",
    "            sentenceObj.addWordToAlternatives(word, finalReplacements)\n",
    "    print(\"Sentiment heavy words:  {0}/{1}\".format(numberOfSentimentHeavyWords, MAX_SENTIMENT_HEAVY_WORDS))\n",
    "    return sentenceObj\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interactionHelper(dictOfAltObjs, sentiments, thisSent, lastWorkingIndex=0):\n",
    "    if(thisSent in sentiments):\n",
    "        mainIndex = sentiments.index(thisSent)\n",
    "        \n",
    "    else:\n",
    "        mainIndex = lastWorkingIndex\n",
    "    \n",
    "    numLower =  mainIndex  # Mod to keep it in range 0 -> val \n",
    "    numHigher = len(sentiments) - mainIndex - 1\n",
    "\n",
    "    print(\"There are totally {0} possible sentences\".format(len(sentiments)))\n",
    "    print(\"Current sentiment is at index {0}, and length of sentiments is {1}\".format(mainIndex, len(sentiments)));\n",
    "    print(\" <--- {0} lower and {1} ---> higher \".format(numLower, numHigher))\n",
    "    print(\"Would you like to see a lower sentiment or higher sentiment or ALL?\")\n",
    "    print(\"Please enter 1 for lower, 2 for higher or 0 for all\")\n",
    "    \n",
    "    targetSentiment = thisSent\n",
    "    textInput = input()\n",
    "    if(textInput == \"\"):\n",
    "        print(\"You entered nothing! exiting\")\n",
    "        return\n",
    "        \n",
    "    if(textInput.isdigit()):\n",
    "        textInput = int(textInput)\n",
    "        if(textInput == 0):\n",
    "            if(VERBOSE_PRINTING): print(\"Printing sentiments from : {0}\".format(sentiments))\n",
    "            for sentiment in sentiments:\n",
    "                obj = dictOfAltObjs[sentiment][0];\n",
    "                displayText(\"{0}  : {1}\".format(obj.getHTML(), sentiment))\n",
    "            return\n",
    "        elif(textInput == 1 and numLower > 0 and (mainIndex - 1) >= 0):\n",
    "            targetSentiment = sentiments[mainIndex - 1]\n",
    "            htmlSentence = dictOfAltObjs[targetSentiment][0].getHTML();\n",
    "            displayText(\"{0} : {1}\".format(htmlSentence, targetSentiment))\n",
    "            \n",
    "        elif(textInput == 2 and numHigher < len(sentiments) and (mainIndex + 1) < len(sentiments)):\n",
    "            targetSentiment = sentiments[mainIndex + 1]\n",
    "            htmlSentence = dictOfAltObjs[targetSentiment][0].getHTML();\n",
    "            displayText(\"{0} : {1}\".format(htmlSentence, targetSentiment))\n",
    "            \n",
    "        else:\n",
    "            print(\"You entered an invalid option\")\n",
    "            interactionHelper(dictOfAltObjs, sentiments, thisSent, mainIndex)\n",
    "            return\n",
    "        \n",
    "    interactionHelper(dictOfAltObjs, sentiments, targetSentiment,mainIndex)\n",
    "\n",
    "\n",
    "\n",
    "def interactionWithUser(mainSentenceObj, dictOfAltObjs,sentiments, isUser=False):\n",
    "    if(isUser == False):\n",
    "        return\n",
    "    else:\n",
    "        mainSentence = mainSentenceObj.ogSentence;\n",
    "        mainSent = mainSentenceObj.ogSentiment;\n",
    "        if(VERBOSE_PRINTING):\n",
    "            print(\"The following are the sentiment values for the input\")\n",
    "            for ind,sent in enumerate(sentiments):\n",
    "                print(\"{0} - {1}\".format(ind+1,sent))\n",
    "        \n",
    "        interactionHelper(dictOfAltObjs, sentiments, mainSent) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 1 Tweet:  Gisburn is great there are some nice little single track. Did you try some of the new bits? I havent been in a while :0.7845\n",
      "\n",
      "Sentiment heavy words:  2/10\n",
      "--- More options (total: 36) possible ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<text style=color:black>Actual Sentence:  Gisburn is great there are some nice little single track. Did you try some of the new bits? I havent been in a while  :0.7845</text>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<text style=color:red>Worst Sentence: Gisburn is <text style=color:blue>good</text> there are some <text style=color:blue>well</text> little single track Did you try some of the new bits? I havent been in a while : 0.6124</text>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<text style=color:green>Best Sentence: Gisburn is <text style=color:blue>great</text> there are some <text style=color:blue>great</text> little single track Did you try some of the new bits? I havent been in a while : 0.8481</text>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 3 Tweet: And hating that watching House will kill me! I can not wait for this next episode and at the same time.. I do not want to see it :-0.8595\n",
      "\n",
      "Sentiment heavy words:  3/10\n",
      "--- More options (total: 150) possible ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<text style=color:black>Actual Sentence: And hating that watching House will kill me! I can not wait for this next episode and at the same time.. I do not want to see it  :-0.8595</text>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<text style=color:red>Worst Sentence: And <text style=color:blue>hating</text> that watching House will <text style=color:blue>kill</text> me! I can not wait for this next episode and at the same time I do not <text style=color:blue>desire</text> to see it : -0.8898</text>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<text style=color:green>Best Sentence: And <text style=color:blue>complaining</text> that watching House will <text style=color:blue>defeat</text> me! I can not wait for this next episode and at the same time I do <text style=color:blue>wish</text> to see it : -0.3382</text>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num of tweets done : 3\n"
     ]
    }
   ],
   "source": [
    "def extractTwitterDataset():\n",
    "#     df_tweets = pd.read_csv( pathToDatasets + 'cleanedTweets.csv', nrows=NUM_OF_TWEETS, skiprows =TWEET_START)\n",
    "    df_tweets = pd.read_csv( pathToDatasets + \"{0}.csv\".format(DATASET_FILE_NAME), nrows=NUM_OF_TWEETS, skiprows =TWEET_START)\n",
    "    tweets = df_tweets.values\n",
    "    return tweets;\n",
    "\n",
    "def createDictionaryOfSentStrings(sentencesWithSentiment):\n",
    "    dict_sentimentToStringObjects = {}\n",
    "    for obj in sentencesWithSentiment:\n",
    "        sent = obj.getSentiment()\n",
    "        if(dict_sentimentToStringObjects.get(sent) == None):\n",
    "            dict_sentimentToStringObjects[sent] = [obj]\n",
    "        else:\n",
    "            (dict_sentimentToStringObjects[sent]).append(obj)\n",
    "\n",
    "    return dict_sentimentToStringObjects;\n",
    "\n",
    "\n",
    "def runThroughTweets():\n",
    "    tweets = extractTwitterDataset()\n",
    "    \n",
    "    counterOfTweets = 0;\n",
    "    for counter,tweet in enumerate(tweets):\n",
    "        counterOfTweets +=1;\n",
    "        tweet = tweet[0]\n",
    "        tweetTokens = cleanAndTokenizeText(tweet)\n",
    "        mainSentiment = senty.polarity_scores(tweet)['compound']\n",
    "        if(mainSentiment == 0):\n",
    "            continue\n",
    "        print(\"\\n\\n {2} Tweet: {0}:{1}\\n\".format(tweet,mainSentiment, counterOfTweets))   \n",
    "        \n",
    "        sentenceObj = Sentence(tweet, mainSentiment)\n",
    "        sentenceObj = getAlternativeWords(sentenceObj)\n",
    "        sentenceObj = editBoosterWords(sentenceObj)\n",
    "        \n",
    "        replacementDictionary = sentenceObj.getDictOfIndexWords();\n",
    "        if(len(replacementDictionary) <= 0):        \n",
    "            print(\" -- No new Strings generated ---\\n\\n\")\n",
    "            continue\n",
    "\n",
    "        keysToChange = replacementDictionary.keys();\n",
    "        if(VERBOSE_PRINTING):\n",
    "            for key in keysToChange:\n",
    "                if(key not in range(0,len(tweetTokens))): continue\n",
    "                print(\"{0}'th word's ({2}) options: {1}\".format(key,replacementDictionary[key],tweetTokens[key]))\n",
    "\n",
    "        sentenceObj = returnCombinationsOfStrings(sentenceObj)\n",
    "        sentencesWithSentiment, sentenceObj = printStrings(sentenceObj, counterOfTweets)\n",
    "       \n",
    "        dict_sentimentToStringObjects = createDictionaryOfSentStrings(sentencesWithSentiment)\n",
    "        \n",
    "        sentiments = sorted(dict_sentimentToStringObjects.keys())\n",
    "        \n",
    "    return counterOfTweets;\n",
    "        \n",
    "\n",
    "numOfTweets = runThroughTweets()\n",
    "print(\"num of tweets done : {0}\".format(numOfTweets))\n",
    "\n",
    "# global LIST_OF_EVAL_OBJECTS\n",
    "\n",
    "# with open(\"eval.json\", \"w+\") as evalWriter:\n",
    "#         jsonString = json.dumps([obj.__dict__ for obj in LIST_OF_EVAL_OBJECTS])\n",
    "#         evalWriter.write(jsonString)\n",
    "# print(\"Eval writer written!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "specificWord = \"good\"\n",
    "def testOneWord(word=\"\"):\n",
    "    if(word==\"\"):\n",
    "        return\n",
    "    print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter a string to use\n",
      "I hope my thesis goes well\n",
      "\n",
      " I hope my thesis goes well:0.6124\n",
      "\n",
      "Sentiment heavy words:  2/10\n",
      "--- More options (total: 30) possible ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<text style=color:black>Actual Sentence: I hope my thesis goes well :0.6124</text>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<text style=color:red>Worst Sentence: I <text style=color:blue>well</text> my thesis goes <text style=color:blue>well</text> : 0.4939</text>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<text style=color:green>Best Sentence: I <text style=color:blue>trust</text> my thesis goes <text style=color:blue>advantageously</text> : 0.7351</text>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.debugger import set_trace\n",
    "\n",
    "\n",
    "specificString = \"\"\n",
    "def specificString(textString=\"\"):\n",
    "    if(textString == \"\" or textString == None):\n",
    "        return\n",
    "    textString = dataClean(textString)\n",
    "    textTokens = cleanAndTokenizeText(textString)\n",
    "    mainSentiment = senty.polarity_scores(textString)['compound']\n",
    "    if(mainSentiment == 0):\n",
    "        print(\"{} \\n No sentiment found in sentence\".format(textString));\n",
    "        return;\n",
    "    print(\"\\n {0}:{1}\\n\".format(textString,mainSentiment))   \n",
    "#     for word in textTokens:\n",
    "#         print(\"{} : {}\".format(word, senty.polarity_scores(word)['compound']))\n",
    "    \n",
    "    \n",
    "    sentenceObj = Sentence(textString, mainSentiment)\n",
    "    sentenceObj = getAlternativeWords(sentenceObj)\n",
    "    sentenceObj = editBoosterWords(sentenceObj)\n",
    "    \n",
    "    \n",
    "    replacementDictionary = sentenceObj.getDictOfIndexWords();\n",
    "    if(len(replacementDictionary) <= 0):        \n",
    "        print(\" -- No new Strings generated ---\\n\\n\")\n",
    "        return\n",
    "    \n",
    "    keysToChange = replacementDictionary.keys();\n",
    "    if(VERBOSE_PRINTING):\n",
    "        for key in keysToChange:\n",
    "               print(\"{0}'th word's ({2}) options: {1}\".format(key,replacementDictionary[key],textTokens[key]))\n",
    "    \n",
    "    sentenceObj = returnCombinationsOfStrings(sentenceObj)\n",
    "    allPossibleSentences, sentenceObj = printStrings(sentenceObj)\n",
    "    \n",
    "print(\"Enter a string to use\")\n",
    "\n",
    "\n",
    "inputText = \"\"\n",
    "inputText = input()\n",
    "if(inputText == \"\"):\n",
    "    return\n",
    "elif(inputText == \"t\"):\n",
    "    specificString(\"And hating that watching House will kill me! I can not wait for this next episode and at the same time.. I do not want to see it\");\n",
    "#     specificString(\"damn the grind is inspirational and saddening at the same time. do not want you to stop cuz i like what u do! much love\")\n",
    "#     specificString(\"I really dislike iced tea, but enjoy iced coffee\")\n",
    "#     specificString(\"I am gonna get up late tomorrow and it is 132am here. I gonna get tipsy by my lonesome. Thats...thats just sad\")\n",
    "#     specificString(\"i hope they will increase the capacity fast yesterday was such a pain. got the fail whale 15 times in 2 hours....\");\n",
    "else:\n",
    "    specificString(inputText)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
