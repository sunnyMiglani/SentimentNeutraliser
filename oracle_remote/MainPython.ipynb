{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:85% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gensim.downloader as api\n",
    "import numpy as np\n",
    "import re\n",
    "import csv\n",
    "import pandas as pd\n",
    "import pprint\n",
    "import string\n",
    "import nltk\n",
    "import sys\n",
    "import spacy\n",
    "import os\n",
    "import pickle\n",
    "import random     \n",
    "import json\n",
    "\n",
    "from IPython.display import HTML\n",
    "from nltk.corpus import wordnet \n",
    "from sklearn.manifold import TSNE\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "pathToDatasets = '../datasets/'\n",
    "pathToDataScripts = '../datasets/scripts/'\n",
    "filePath = '../datasets/GoogleNews-vectors-negative300.bin'\n",
    "# modelBeingUsed = \"glove-wiki-gigaword-300\"\n",
    "# modelBeingUsed = \"glove-wiki-gigaword-100\"\n",
    "# modelBeingUsed = \"glove-twitter-50\"\n",
    "modelBeingUsed = \"glove-twitter-100\"\n",
    "# modelBeingUsed = \"glove-twitter-200\"\n",
    "\n",
    "# DATASET_FILE_NAME = \"shuffled_twitter_corpus_output\"\n",
    "# DATASET_FILE_NAME = \"airplane_tweets\"\n",
    "DATASET_FILE_NAME = \"cleanedTweets\"\n",
    "\n",
    "\n",
    "\n",
    "sys.path.insert(0, pathToDataScripts)\n",
    "from cleanDataset import tokenize_words, dataClean\n",
    "\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:85% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading binaries and models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Should I reload the model?\n",
      "no\n",
      " didnt reload model! \n",
      "{'definitely', 'incredibly', 'extremely', 'absolutely', 'especially', 'completely', 'overwhelmingly'}\n"
     ]
    }
   ],
   "source": [
    "print(\"Should I reload the model?\")\n",
    "tstString = input()\n",
    "if(\"no\" in tstString.lower() or \"n\" in tstString.lower()):\n",
    "    print(\" didnt reload model! \")\n",
    "else:\n",
    "    print(\"loading {0}!\".format(modelBeingUsed));\n",
    "    fileName = \"{}.pickle\".format(modelBeingUsed)\n",
    "    if(os.path.exists(pathToDatasets+fileName)):\n",
    "        print(\"loading via pickle!\")\n",
    "        pickle_in = open(pathToDatasets+fileName, \"rb\")\n",
    "        word_vectors = pickle.load(pickle_in);\n",
    "    else:\n",
    "        print(\"Pickle didn't exist, therefore loading model!\")\n",
    "        word_vectors = api.load(\"{0}\".format(modelBeingUsed))\n",
    "        print(\"-- Saving to pickle file for next time! --\")\n",
    "        pickle_out = open(pathToDatasets+fileName,\"wb\")\n",
    "        pickle.dump(word_vectors, pickle_out)\n",
    "        pickle_out.close()\n",
    "        \n",
    "    nltk.download('vader_lexicon')\n",
    "    nltk.download('punkt')\n",
    "    nltk.download('averaged_perceptron_tagger')\n",
    "    nltk.download('wordnet')\n",
    "    print(\"Model Loaded!\")\n",
    "\n",
    "setOfBoosterWords = set()\n",
    "with open(pathToDatasets + \"BoosterWordList.txt\") as bwf:\n",
    "    pd_booster = pd.read_csv(bwf, sep='\\t');\n",
    "    listOfBoosterWords = (pd_booster.iloc[:,0]).tolist() \n",
    "    setOfBoosterWords = set(listOfBoosterWords[:7])\n",
    "print(setOfBoosterWords)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global Variables and Global Objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "senty = SentimentIntensityAnalyzer()\n",
    "vocabulary = word_vectors.vocab;\n",
    "\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "\n",
    "'''\n",
    "Good sets -> {100+20}\n",
    "'''\n",
    "TWEET_START = 200\n",
    "NUM_OF_TWEETS = 100\n",
    "\n",
    "VERBOSE_PRINTING = True\n",
    "# VERBOSE_PRINTING = False\n",
    "\n",
    "# USE_SPACY = False\n",
    "USE_SPACY = True\n",
    "\n",
    "# COLOR_PRINTING = True\n",
    "COLOR_PRINTING = False\n",
    "\n",
    "PRINT_NEUTRAL = True\n",
    "# PRINT_NEUTRAL = False\n",
    "\n",
    "# PRINT_ALL_STRINGS = True\n",
    "PRINT_ALL_STRINGS = False\n",
    "\n",
    "HANDLE_NEGATIONS = True\n",
    "# HANDLE_NEGATIONS = False\n",
    "\n",
    "HANDLE_BOOSTERS = True\n",
    "# HANDLE_BOOSTERS = False\n",
    "\n",
    "KEEP_SENSE = True\n",
    "# KEEP_SENSE = False\n",
    "\n",
    "SHOW_ALTS = 35\n",
    "\n",
    "NUMBER_OF_NEAR = 10\n",
    "MAX_SENTIMENT_HEAVY_WORDS = 6\n",
    "MAX_REPLACEMENTS = 8 # increasing this has a x(NUM_OF_SENTIMENT_WORDS) effect.\n",
    "\n",
    "PROB_ADDING_BOOSTER = 0.40 # Keep between 0 and 1, if you want all possible, use 1. If you want none, use 0\n",
    "\n",
    "\n",
    "punctuation = r\"\\\"#$%&'+-/;<=>@[\\]*^_`{|}~\"\n",
    "LIST_OF_NEGATIONS = [\"not\", \"no\"]\n",
    "BANNED_WORDS = [\"fuck\",\"bitch\", \"motherfucker\"]\n",
    "\n",
    "LIST_OF_EVAL_OBJECTS = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class for Sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from SentenceClass import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EvaluationObj:\n",
    "    def __init__(self, main, best, worst, mainSent, bestSent, worstSent, uniqueID):\n",
    "        self.main  = main\n",
    "        self.best = best\n",
    "        self.worst = worst\n",
    "        self.mainSent = mainSent\n",
    "        self.bestSent = bestSent\n",
    "        self.worstSent = worstSent\n",
    "        self.uniqueID = uniqueID\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printStrings(sentenceObj, uniqueID = -1):\n",
    "    \n",
    "    \n",
    "    numberOfPrints = 0\n",
    "    newStrings = generateHTMLObjectsFromSentence(sentenceObj)\n",
    "    mainSentiment = sentenceObj.ogSentiment;\n",
    "    listOfSentencesWithSentiments = []\n",
    "    bestSentiment = -sys.maxsize - 1\n",
    "    worstSentiment = sys.maxsize\n",
    "    \n",
    "    isPositiveSentence = True if(mainSentiment >= 0) else False\n",
    "    \n",
    "    bestSentimentString = \"\";\n",
    "    worstSentimentString = \"\";\n",
    "    \n",
    "    if(uniqueID != -1):\n",
    "        global LIST_OF_EVAL_OBJECTS\n",
    "    \n",
    "    numberOfSentimentHeavyWordsYet = 0;\n",
    "    for ind, tSentence in enumerate(newStrings):\n",
    "            alteredTweet = tSentence.getSentence()\n",
    "            htmlText = tSentence.getHTML()\n",
    "            numberOfPrints+=1;\n",
    "            \n",
    "            \n",
    "            sentimentOfNewString = tSentence.getSentiment() #senty.polarity_scores(alteredTweet)['compound']\n",
    "            \n",
    "            thisStringPositive = True if(sentimentOfNewString >= 0) else False\n",
    "            if(thisStringPositive != isPositiveSentence and KEEP_SENSE):\n",
    "                continue\n",
    "            \n",
    "            newObj = SentenceWithSentiment(alteredTweet, sentimentOfNewString, htmlText)\n",
    "            listOfSentencesWithSentiments.append(newObj)\n",
    "            \n",
    "            if(sentimentOfNewString >= bestSentiment):\n",
    "                bestSentiment = sentimentOfNewString\n",
    "                bestSentimentString = htmlText;\n",
    "            elif(sentimentOfNewString < worstSentiment):\n",
    "                worstSentiment = sentimentOfNewString\n",
    "                worstSentimentString = htmlText;\n",
    "            \n",
    "            \n",
    "#             if(numberOfPrints > SHOW_ALTS): break\n",
    "            if(numberOfPrints > SHOW_ALTS or PRINT_ALL_STRINGS == False): continue\n",
    "            if(sentimentOfNewString == mainSentiment or sentimentOfNewString == 0.0):\n",
    "                if(PRINT_ALL_STRINGS and numberOfPrints <= SHOW_ALTS): displayText(\"{0}: {1}\".format(htmlText,sentimentOfNewString),'black')\n",
    "            elif(sentimentOfNewString > mainSentiment):\n",
    "                if(PRINT_ALL_STRINGS and numberOfPrints <= SHOW_ALTS): displayText(\"{0}: {1}\".format(htmlText,sentimentOfNewString),'green')\n",
    "            elif(sentimentOfNewString < mainSentiment and sentimentOfNewString != 0.0):\n",
    "                if(PRINT_ALL_STRINGS and numberOfPrints <= SHOW_ALTS): displayText(\"{0}: {1}\".format(htmlText,sentimentOfNewString),'red')\n",
    "                \n",
    "#     if(numberOfPrints > SHOW_ALTS): print(\"--- More options (total: {0}) possible, but not printed ---\".format(numberOfPrints));\n",
    "    print(\"--- More options (total: {0}) possible ---\".format(numberOfPrints));\n",
    "    displayText(\"Actual Sentence: {0} :{1}\".format(sentenceObj.ogSentence, sentenceObj.ogSentiment))\n",
    "    if (worstSentimentString != \"\"): displayText(\"Worst Sentence: {0} : {1}\".format(worstSentimentString, worstSentiment), color='red')\n",
    "    if (bestSentimentString != \"\"): displayText(\"Best Sentence: {0} : {1}\".format(bestSentimentString, bestSentiment), color='green') \n",
    "        \n",
    " \n",
    "        \n",
    "    if(worstSentimentString != \"\" and bestSentimentString != \"\" and uniqueID != -1):\n",
    "#             file_writeAll.write(\"Inp({3}): {0}\\nB({3}): {1}\\nW({3}): {2}\\nInSent:{4}, BSent:{5}, WSent:{6}\\n\\n\".format(sentenceObj.ogSentence, bestSentimentString, worstSentimentString, uniqueID, sentenceObj.ogSentiment, bestSentiment, worstSentiment));\n",
    "            thisEvalObj = EvaluationObj(sentenceObj.ogSentence, bestSentimentString, worstSentimentString, sentenceObj.ogSentiment, bestSentiment, worstSentiment, uniqueID);\n",
    "            LIST_OF_EVAL_OBJECTS.append(thisEvalObj)\n",
    "            \n",
    "            print(\"Wrote something!\")\n",
    "            \n",
    "\n",
    "    sentenceObj.addFinalSentences(listOfSentencesWithSentiments)\n",
    "    return listOfSentencesWithSentiments, sentenceObj;\n",
    "\n",
    "def cstr(s, color='black', italics=False):\n",
    "    if(COLOR_PRINTING):\n",
    "        if(italics):\n",
    "            return cstr(\"{0}\".format(s), color);\n",
    "        return \"<text style=color:{}>{}</text>\".format(color, s)\n",
    "    else:\n",
    "        return \"{}\".format(s)\n",
    "\n",
    "def displayText(text, color='black'):\n",
    "    if(COLOR_PRINTING):\n",
    "        display(HTML(cstr(text, color)));\n",
    "        return\n",
    "    print(\"{}\".format(text));\n",
    "    \n",
    "    \n",
    "def cleanAndTokenizeText(text):\n",
    "#     text = text.lower();\n",
    "    newString = \"\"\n",
    "    for char in text:\n",
    "        if char not in punctuation:\n",
    "            newString += char\n",
    "    text = word_tokenize(newString)\n",
    "    return text;\n",
    "\n",
    "def getPOSTags(tweet):\n",
    "    if(USE_SPACY == False):\n",
    "        tags = nltk.pos_tag(tweet)\n",
    "        return tags;    \n",
    "    tweet = ' '.join(tweet)\n",
    "    doc = nlp(tweet)\n",
    "    tags = [(token.text, token.tag_) for token in doc] # since the format expected is [text,tag]\n",
    "    return tags\n",
    "\n",
    "def getAntonymsAndSynonymsOfWords(word):\n",
    "    if(word not in vocabulary):\n",
    "        return [], []\n",
    "    setOfAntonyms = set()\n",
    "    setOfSynonyms = set()\n",
    "    for syn in wordnet.synsets(word):\n",
    "        for l in syn.lemmas():\n",
    "            setOfSynonyms.add(l.name().lower())\n",
    "            anton = l.antonyms()\n",
    "            if(anton!=[]):\n",
    "                setOfAntonyms.add(anton[0].name().lower())\n",
    "    if(len(setOfAntonyms) == 0):\n",
    "        if(VERBOSE_PRINTING): print(\"No antonyms found for word {0}\".format(word))\n",
    "    if(len(setOfSynonyms) == 0):\n",
    "        if(VERBOSE_PRINTING): print(\"No synonyms found for word {0}\".format(word))\n",
    "            \n",
    "    if(KEEP_SENSE):\n",
    "        return [], list(setOfSynonyms)\n",
    "    else:    \n",
    "        return list(setOfAntonyms), list(setOfSynonyms)\n",
    "\n",
    "\n",
    "def returnReplacementsForWord(word):\n",
    "    \n",
    "    if(word not in vocabulary):\n",
    "        print(\" --- {0} not in vocabulary ---\".format(word))\n",
    "        return []\n",
    "    possibleReplacements = [word[0] for word in word_vectors.most_similar(word,topn=NUMBER_OF_NEAR)]\n",
    "    \n",
    "    if(possibleReplacements == []):\n",
    "        print(\" --- No replacements for word {0} ---\".format(word))\n",
    "    antonyms,synonyms = getAntonymsAndSynonymsOfWords(word)\n",
    "    \n",
    "    if(antonyms != []):\n",
    "        possibleReplacements.extend(antonyms)\n",
    "        if(VERBOSE_PRINTING): print(\"Some antonyms for word {0} are {1}\".format(word, antonyms[:3]))\n",
    "    if(synonyms != []):\n",
    "        possibleReplacements.extend(synonyms)\n",
    "        if(VERBOSE_PRINTING): print(\"Some synonyms for word {0} are {1}\".format(word, synonyms[:3]))\n",
    "    return possibleReplacements\n",
    "    \n",
    "def posApprovedReplacements(alternativeWords, userTokens, indexOfToken):\n",
    "    if(alternativeWords == []):\n",
    "        return []\n",
    "    tempTokens = userTokens[:]\n",
    "    POSTokens = getPOSTags(tempTokens)\n",
    "    validWords = []\n",
    "    \n",
    "    mainTag = POSTokens[indexOfToken][1]\n",
    "    mainWord = userTokens[indexOfToken]\n",
    "    \n",
    "    for ind,word in enumerate(alternativeWords):\n",
    "        if(\"_\" in word): continue\n",
    "        tempTokens[indexOfToken] = word\n",
    "        posTags = getPOSTags(tempTokens)\n",
    "        newTag = (posTags[indexOfToken])[1]\n",
    "        if(str(newTag) == str(mainTag)):\n",
    "            validWords.append(word)\n",
    "    if(validWords == [] and VERBOSE_PRINTING):\n",
    "        print(\"No POS words found for word {} with tag {}\".format(mainWord, mainTag));\n",
    "    return validWords\n",
    "    \n",
    "def editBoosterWords(sentenceObj):\n",
    "    if(HANDLE_BOOSTERS == False): return sentenceObj;\n",
    "    ogSentence = sentenceObj.ogSentence;\n",
    "    \n",
    "    tokenizedSentence = sentenceObj.getSentenceTokens()\n",
    "    \n",
    "    '''\n",
    "        Use the indexToAlternatives dictionary to add words into the sentences, as part of the chunking process \n",
    "        The problem with this instinctively is how do you have an index \"between\" two variables \n",
    "        You can't really. You'd have to have a flag that says which places it's meant to go \n",
    "        You could do it _before_ the sentiment heavy word (this makes more natural sense) or \n",
    "            you could do it _after_ the word before the sentiment heavy word (this could have its own advantages) \n",
    "    '''\n",
    "    \n",
    "    posTagsForWords = getPOSTags(tokenizedSentence)    \n",
    "    for ind,(word,tag) in enumerate(posTagsForWords):\n",
    "        tmp = ' '.join(tokenizedSentence[ind:ind+2])    \n",
    "        if(word in setOfBoosterWords):\n",
    "            if(senty.polarity_scores(tmp)['compound'] != 0):\n",
    "                sentenceObj.addAlternativesByIndex(ind, [\"IGNORE_FLAG\", word]);\n",
    "                continue\n",
    "        elif(senty.polarity_scores(word)['compound'] != 0 and (\"RB\" in tag or \"JJ\" in tag or \"VB\" in tag)):  # \"JJ\" in tag and\n",
    "            newInd = ind-1 if ((ind-1) >= 0 ) else 0\n",
    "            randNum = random.uniform(0,1)\n",
    "            if(randNum < PROB_ADDING_BOOSTER):\n",
    "                sentenceObj.addAlternativesByIndex(newInd, [\"BOOSTER_FLAG\"]);\n",
    "#                 if(VERBOSE_PRINTING): print(\"Placed booster at {0} from word {1} on word {2}\".format(newInd, word,tokenizedSentence[newInd] ))\n",
    "    \n",
    "    return sentenceObj\n",
    "\n",
    "\n",
    "def generateHTMLObjectsFromSentence(sentenceObj):\n",
    "    \n",
    "    allSentences = sentenceObj.getFinalSentences()\n",
    "    indexToAlts = sentenceObj.indexToSetOfWords;\n",
    "    indexToChange = list(indexToAlts.keys());\n",
    "    \n",
    "    numberOfNegationsRemoved = 0;\n",
    "    listOfSentenceObjs = []\n",
    "    for t_sentenceObj in allSentences:\n",
    "        sentence = t_sentenceObj.getSentence()\n",
    "        copySentence = t_sentenceObj.getSentenceTokens()\n",
    "        subIndex = 0\n",
    "        addIndex = 0\n",
    "        for index in sorted(indexToChange):\n",
    "            if(index > len(copySentence)):  \n",
    "                copySentence[len(copySentence) - index] = cstr(\"{0}\".format(copySentence[len(copySentence) - index]), \"blue\", italics=True);\n",
    "                continue\n",
    "            else:\n",
    "                if(\"IGNORE_FLAG\" in indexToAlts[index]):\n",
    "                    if(copySentence[index] in LIST_OF_NEGATIONS):\n",
    "                        continue; ## Word that _should_ be removed is present.\n",
    "                    else:\n",
    "                        subIndex +=1\n",
    "                        numberOfNegationsRemoved +=1;\n",
    "                        continue;\n",
    "                elif(\"BOOSTER_FLAG\" in indexToAlts[index]):\n",
    "                    addIndex +=1;\n",
    "\n",
    "            newIndex = index + addIndex - subIndex;\n",
    "            if(newIndex >= len(copySentence)):\n",
    "                newIndex = len(copySentence)-1\n",
    "            elif(newIndex < 0):\n",
    "                newIndex = 0;\n",
    "            \n",
    "            \n",
    "            copySentence[newIndex] = cstr(\"{0}\".format(copySentence[newIndex]), \"blue\", italics=True);\n",
    "        listOfSentenceObjs.append(SentenceWithHTML(sentence, ' '.join(copySentence), t_sentenceObj.getSentiment()));\n",
    "    \n",
    "    if(VERBOSE_PRINTING): print(\"Number of negations removed : {0}\".format(numberOfNegationsRemoved))\n",
    "    return listOfSentenceObjs\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence Chunking and Appending"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def helper_combine(mainList, myList):\n",
    "    '''\n",
    "    helper function for CombineSentenceChunks\n",
    "    '''\n",
    "    newList = []\n",
    "    for val in myList:\n",
    "        for mainVal in mainList:\n",
    "#             if(VERBOSE_PRINTING): print(\"Combining {0} with {1}\".format(' '.join(val), ' '.join(mainVal)));\n",
    "            newList.append(val + mainVal);\n",
    "    return newList;\n",
    "\n",
    "def combineSentenceChunks(wholeSentence, dictOfChunks):\n",
    "    '''\n",
    "        Uses the helper_combine function to generate all possible combinatios and permuations of the chunks\n",
    "        and any alternatives.\n",
    "        \n",
    "        The logic is to use the end of the sentence, apply each possible chunk from the previous key's chunks\n",
    "        to every possible chunk of this key's.\n",
    "        \n",
    "        The helper function is used to allow us to reuse the list of alreadyGeneratedChunks and constantly\n",
    "        append to them.\n",
    "        \n",
    "        To understand the logic better, take a look at this gist:\n",
    "        https://gist.github.com/sunnyMiglani/cf85407a9e6928237b1436cc2bc95fa4\n",
    "        \n",
    "    '''\n",
    "    reversedKeys = sorted(dictOfChunks.keys(), reverse=True)\n",
    "    completeSentences = [];\n",
    "    mainArr = dictOfChunks[reversedKeys[0]]\n",
    "    for ind in range(1, len(reversedKeys)):\n",
    "        key = reversedKeys[ind];\n",
    "        mainArr = helper_combine(mainArr, dictOfChunks[key]);\n",
    "        \n",
    "    return mainArr;\n",
    "        \n",
    "def generateSentenceChunks(wholeSentence, keyToChange, nextKey, listOfMyAlternatives):\n",
    "    '''\n",
    "        Generates sentence chunks by iterating through the list of alternatives\n",
    "        Chunking the sentence to start from current key to next key.\n",
    "        This means that the sentence always goes from key 'x' to key 'y'\n",
    "        \n",
    "        Example:\n",
    "        \"I really <hate> hot chocolate, but I <prefer> hot coffee\"\n",
    "        Calling generateSentenceChunks will create an example sentence:\n",
    "            - \"<altWordForHate> hot chocolate , but I \"\n",
    "        \n",
    "        Remember to append the first stretch of the string to the first key's chunk for proper use!\n",
    "    '''\n",
    "    \n",
    "    newList = list(listOfMyAlternatives)\n",
    "    if(keyToChange < len(wholeSentence) and keyToChange >= 0):\n",
    "        newList.append(wholeSentence[keyToChange]);\n",
    "    else:\n",
    "        return []\n",
    "    generatedSentences = []\n",
    "    \n",
    "        \n",
    "    for myAlt in newList:\n",
    "        if(\"IGNORE_FLAG\" == myAlt):\n",
    "            newSentence = wholeSentence[keyToChange+1:nextKey];\n",
    "            generatedSentences.append(newSentence)\n",
    "            continue\n",
    "        elif(\"BOOSTER_FLAG\" == myAlt): \n",
    "            for booster in setOfBoosterWords:\n",
    "                newSentence = wholeSentence[:]\n",
    "                newSentence.insert(keyToChange+1, booster)\n",
    "                generatedSentences.append(newSentence[keyToChange:nextKey+1]);\n",
    "        else:\n",
    "            newSentence = wholeSentence[:]\n",
    "            newSentence[keyToChange] = myAlt\n",
    "            generatedSentences.append(newSentence[keyToChange:nextKey]);\n",
    "        \n",
    "    return generatedSentences\n",
    "    \n",
    "def returnCombinationsOfStrings(sentenceObj):\n",
    "    \n",
    "    indexToWordDict = sentenceObj.indexToSetOfWords;\n",
    "    originalSentence = sentenceObj.ogSentence;\n",
    "    tokenizedSentence = sentenceObj.getSentenceTokens() \n",
    "    reversedKeys = sorted(indexToWordDict.keys(), reverse=True)\n",
    "    dictAlternatives  = {}\n",
    "\n",
    "    sortedKeys = sorted(indexToWordDict.keys())\n",
    "    sentenceChunks = {}\n",
    "    htmlChunks = {}\n",
    "    \n",
    "    for ind in range(0,len(sortedKeys)):\n",
    "        key = sortedKeys[ind]\n",
    "        nextKey = sortedKeys[ind+1] if ind+1 < len(sortedKeys) else len(tokenizedSentence)\n",
    "        sentenceChunks[key] = generateSentenceChunks(tokenizedSentence, key, nextKey, indexToWordDict[key])\n",
    "\n",
    "    if(sortedKeys[0] != 0):\n",
    "        newList = []\n",
    "        for thislist in sentenceChunks[sortedKeys[0]]:\n",
    "            newList.append(tokenizedSentence[0:sortedKeys[0]] + thislist)\n",
    "        sentenceChunks[sortedKeys[0]] = newList;\n",
    "        \n",
    "    finalOptions = combineSentenceChunks(tokenizedSentence, sentenceChunks)\n",
    "    \n",
    "    finalSentences = []\n",
    "    for val in finalOptions:\n",
    "        sentence = ' '.join(val)\n",
    "        sentiment = senty.polarity_scores(sentence)['compound']\n",
    "        newSentenceObj = SentenceWithSentiment(sentence,sentiment)\n",
    "        newSentenceObj.setSentenceTokens(val)\n",
    "        finalSentences.append(newSentenceObj)\n",
    "    \n",
    "    sentenceObj.addFinalSentences(finalSentences)   \n",
    "    return sentenceObj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trimSuggestions(sentenceObj, mainWord, listOfAlternatives):\n",
    "    mainSentiment = senty.polarity_scores(mainWord)['compound']\n",
    "    isPositive = True if(mainSentiment > 0) else False\n",
    "    finalWords = []\n",
    "    for word in listOfAlternatives:\n",
    "        if(word) in BANNED_WORDS: continue\n",
    "        senty_suggestion = senty.polarity_scores(word)['compound'];\n",
    "        isPositive_suggestion = True if senty_suggestion > 0 else False\n",
    "        if(KEEP_SENSE and isPositive_suggestion != isPositive): continue\n",
    "        if(senty_suggestion == 0):\n",
    "            continue\n",
    "        else:\n",
    "            finalWords.append(word)\n",
    "    if(len(finalWords) == 1):\n",
    "        if(finalWords[0] == mainWord): return []\n",
    "    return finalWords\n",
    "\n",
    "def getAlternativeWords(sentenceObj):\n",
    "    mainSentence = sentenceObj.ogSentence;\n",
    "    mainSentiment = sentenceObj.ogSentiment;\n",
    "\n",
    "    sentenceTokens = cleanAndTokenizeText(mainSentence)\n",
    "    sentenceObj.setSentenceTokens(sentenceTokens)\n",
    "\n",
    "    numberOfSentimentHeavyWords = 0;\n",
    "    for ind, word in enumerate(sentenceTokens):\n",
    "        alternativeSentenceWithHTML = []\n",
    "        copyOfTokens = sentenceTokens[:]\n",
    "        replacements = []\n",
    "        replacements = sentenceObj.checkIfWordExists(word)\n",
    "        if(replacements != []):\n",
    "            print(\"FOUND REPEATED WORD {0}\".format(word))\n",
    "            sentenceObj.addAlternativesByIndex(ind, replacements)\n",
    "            continue;\n",
    "        \n",
    "        if(HANDLE_NEGATIONS):\n",
    "            if(word in LIST_OF_NEGATIONS):\n",
    "                \n",
    "                '''\n",
    "                    If the word is in the list of negations\n",
    "                    We add a [IGNORE_FLAG] tag to the alternatives\n",
    "                    Which will be grabbed in the future by the sentence generation algorithm. (chunking)\n",
    "                '''\n",
    "                \n",
    "                tmp = ' '.join(sentenceTokens[ind:ind+3])\n",
    "                if(senty.polarity_scores(tmp)['compound'] != 0):\n",
    "                    sentenceObj.addAlternativesByIndex(ind, [\"IGNORE_FLAG\", word]);\n",
    "                    continue\n",
    "\n",
    "        score = senty.polarity_scores(word)['compound']\n",
    "        if(score != 0.0):\n",
    "            if(numberOfSentimentHeavyWords < MAX_SENTIMENT_HEAVY_WORDS):\n",
    "                numberOfSentimentHeavyWords+=1;\n",
    "            else:\n",
    "                print(\"Sentiment heavy words:  {0}/{1}\".format(numberOfSentimentHeavyWords, MAX_SENTIMENT_HEAVY_WORDS))\n",
    "                return sentenceObj\n",
    "            \n",
    "            tempReplacements = returnReplacementsForWord(word) # get embedding based relations\n",
    "            \n",
    "            if(VERBOSE_PRINTING): print(\"Some early replacements for word {0} are {1}\".format(word, tempReplacements[0:8]))\n",
    "            if(tempReplacements == []):\n",
    "                print(\"No replacements found at all for word {0}\".format(word))\n",
    "                continue\n",
    "            \n",
    "            replacements = posApprovedReplacements(tempReplacements[:], copyOfTokens[:], ind)\n",
    "            finalReplacements = trimSuggestions(sentenceObj,word, replacements)\n",
    "            \n",
    "            if(finalReplacements == []):\n",
    "                if(VERBOSE_PRINTING): print(\" -- No POS approved words after trimming! -- for word {0}\\n some non-trimmed:{1}\".format(word, tempReplacements[:4]))\n",
    "                continue\n",
    "            \n",
    "            # Trimming until max replacements to ensure efficiency\n",
    "            finalReplacements = finalReplacements[:MAX_REPLACEMENTS]\n",
    "            sentenceObj.addAlternativesByIndex(ind, finalReplacements)\n",
    "            sentenceObj.addWordToAlternatives(word, finalReplacements)\n",
    "    print(\"Sentiment heavy words:  {0}/{1}\".format(numberOfSentimentHeavyWords, MAX_SENTIMENT_HEAVY_WORDS))\n",
    "    return sentenceObj\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interactionHelper(dictOfAltObjs, sentiments, thisSent, lastWorkingIndex=0):\n",
    "    if(thisSent in sentiments):\n",
    "        mainIndex = sentiments.index(thisSent)\n",
    "        \n",
    "    else:\n",
    "        mainIndex = lastWorkingIndex\n",
    "    \n",
    "    numLower =  mainIndex  # Mod to keep it in range 0 -> val \n",
    "    numHigher = len(sentiments) - mainIndex - 1\n",
    "\n",
    "    print(\"There are totally {0} possible sentences\".format(len(sentiments)))\n",
    "    print(\"Current sentiment is at index {0}, and length of sentiments is {1}\".format(mainIndex, len(sentiments)));\n",
    "    print(\" <--- {0} lower and {1} ---> higher \".format(numLower, numHigher))\n",
    "    print(\"Would you like to see a lower sentiment or higher sentiment or ALL?\")\n",
    "    print(\"Please enter 1 for lower, 2 for higher or 0 for all\")\n",
    "    \n",
    "    targetSentiment = thisSent\n",
    "    textInput = input()\n",
    "    if(textInput == \"\"):\n",
    "        print(\"You entered nothing! exiting\")\n",
    "        return\n",
    "        \n",
    "    if(textInput.isdigit()):\n",
    "        textInput = int(textInput)\n",
    "        if(textInput == 0):\n",
    "            if(VERBOSE_PRINTING): print(\"Printing sentiments from : {0}\".format(sentiments))\n",
    "            for sentiment in sentiments:\n",
    "                obj = dictOfAltObjs[sentiment][0];\n",
    "                displayText(\"{0}  : {1}\".format(obj.getHTML(), sentiment))\n",
    "            return\n",
    "        elif(textInput == 1 and numLower > 0 and (mainIndex - 1) >= 0):\n",
    "            targetSentiment = sentiments[mainIndex - 1]\n",
    "            htmlSentence = dictOfAltObjs[targetSentiment][0].getHTML();\n",
    "            displayText(\"{0} : {1}\".format(htmlSentence, targetSentiment))\n",
    "            \n",
    "        elif(textInput == 2 and numHigher < len(sentiments) and (mainIndex + 1) < len(sentiments)):\n",
    "            targetSentiment = sentiments[mainIndex + 1]\n",
    "            htmlSentence = dictOfAltObjs[targetSentiment][0].getHTML();\n",
    "            displayText(\"{0} : {1}\".format(htmlSentence, targetSentiment))\n",
    "            \n",
    "        else:\n",
    "            print(\"You entered an invalid option\")\n",
    "            interactionHelper(dictOfAltObjs, sentiments, thisSent, mainIndex)\n",
    "            return\n",
    "        \n",
    "    interactionHelper(dictOfAltObjs, sentiments, targetSentiment,mainIndex)\n",
    "\n",
    "\n",
    "\n",
    "def interactionWithUser(mainSentenceObj, dictOfAltObjs,sentiments, isUser=False):\n",
    "    if(isUser == False):\n",
    "        return\n",
    "    else:\n",
    "        mainSentence = mainSentenceObj.ogSentence;\n",
    "        mainSent = mainSentenceObj.ogSentiment;\n",
    "        if(VERBOSE_PRINTING):\n",
    "            print(\"The following are the sentiment values for the input\")\n",
    "            for ind,sent in enumerate(sentiments):\n",
    "                print(\"{0} - {1}\".format(ind+1,sent))\n",
    "        \n",
    "        interactionHelper(dictOfAltObjs, sentiments, mainSent) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 1 Tweet: Job Interview in Cardiff today wish me luck! Got about 3 hours sleep :0.7177\n",
      "\n",
      "Some synonyms for word wish are ['indirect_request', 'wishing', 'regard']\n",
      "Some early replacements for word wish are ['hope', 'remember', 'forget', 'have', 'you', 'better', 'think', 'always']\n",
      "No antonyms found for word luck\n",
      "Some synonyms for word luck are ['chance', 'lot', 'luck']\n",
      "Some early replacements for word luck are ['goodluck', 'hope', 'good', 'congrats', 'hopefully', 'thanks', 'well', 'thank']\n",
      "Sentiment heavy words:  2/6\n",
      "5'th word's (wish) options: {'wish', 'hope', 'care', 'want'}\n",
      "7'th word's (luck) options: {'chance', 'luck'}\n",
      "Number of negations removed : 0\n",
      "--- More options (total: 15) possible ---\n",
      "Actual Sentence: Job Interview in Cardiff today wish me luck! Got about 3 hours sleep  :0.7177\n",
      "Worst Sentence: Job Interview in Cardiff today want me chance ! Got about 3 hours sleep : 0.3802\n",
      "Best Sentence: Job Interview in Cardiff today care me luck ! Got about 3 hours sleep : 0.7574\n",
      "Wrote something!\n",
      "\n",
      "\n",
      " 2 Tweet:  Your show is whack. Way worse than whack it is wiggetywhack.    ***:-0.4767\n",
      "\n",
      "Some synonyms for word worse are ['worsened', 'big', 'regretful']\n",
      "Some early replacements for word worse are ['worst', 'awful', 'either', 'probably', 'than', 'reason', 'thing', 'wrong']\n",
      " -- No POS approved words after trimming! -- for word worse\n",
      " some non-trimmed:['worst', 'awful', 'either', 'probably']\n",
      "Sentiment heavy words:  1/6\n",
      "5'th word's (Way) options: {'BOOSTER_FLAG'}\n",
      "Number of negations removed : 0\n",
      "--- More options (total: 8) possible ---\n",
      "Actual Sentence:  Your show is whack. Way worse than whack it is wiggetywhack.    *** :-0.4767\n",
      "Worst Sentence: Your show is whack . Way overwhelmingly worse than whack it is wiggetywhack . : -0.5574\n",
      "Best Sentence: Your show is whack . Way definitely worse than whack it is wiggetywhack . : -0.1027\n",
      "Wrote something!\n",
      "\n",
      "\n",
      " 3 Tweet:  I really do not think people choose to be that way. But I think he chose not to accept my familys help   He might be dead by now:-0.5837\n",
      "\n",
      "Some synonyms for word accept are ['admit', 'accept', 'take']\n",
      "Some early replacements for word accept are ['expect', 'allow', 'willing', 'deny', 'forgive', 'give', 'nor', 'reject']\n",
      "No antonyms found for word help\n",
      "Some synonyms for word help are ['avail', 'supporter', 'help_oneself']\n",
      "Some early replacements for word help are ['need', 'please', 'helping', 'us', 'must', 'let', 'pls', 'give']\n",
      " -- No POS approved words after trimming! -- for word help\n",
      " some non-trimmed:['need', 'please', 'helping', 'us']\n",
      "Some synonyms for word dead are ['bushed', 'suddenly', 'dead']\n",
      "Some early replacements for word dead are ['death', 'walking', 'hell', 'died', 'dying', 'kill', 'killed', 'ghost']\n",
      "Sentiment heavy words:  3/6\n",
      "17'th word's (not) options: {'not', 'IGNORE_FLAG'}\n",
      "19'th word's (accept) options: {'admit', 'accept', 'allow', 'consent', 'forgive'}\n",
      "26'th word's (dead) options: {'numb', 'dead'}\n",
      "18'th word's (to) options: {'BOOSTER_FLAG'}\n",
      "21'th word's (familys) options: {'BOOSTER_FLAG'}\n",
      "25'th word's (be) options: {'BOOSTER_FLAG'}\n",
      "Number of negations removed : 9216\n",
      "--- More options (total: 27648) possible ---\n",
      "Actual Sentence:  I really do not think people choose to be that way. But I think he chose not to accept my familys help   He might be dead by now :-0.5837\n",
      "Worst Sentence: I really do not think people choose to be that way . But I think he chose not to definitely accept my familys definitely help He might be definitely dead by now : -0.8611\n",
      "Best Sentence: I really do not think people choose to be that way . But I think he chose not to overwhelmingly forgive my familys completely help He might be definitely dead by now : -0.0132\n",
      "Wrote something!\n",
      "\n",
      "\n",
      " 4 Tweet:  You are going to kill me but I have not seen DS9. I have been waiting till I can do it in one solid week sitting. :-0.2382\n",
      "\n",
      "No antonyms found for word kill\n",
      "Some synonyms for word kill are ['pop', 'bolt_down', 'kill']\n",
      "Some early replacements for word kill are ['killing', 'let', 'fuck', 'hell', 'kills', 'tell', 'killed', \"n't\"]\n",
      "Some synonyms for word solid are ['solid_state', 'square', 'self-colored']\n",
      "Some early replacements for word solid are ['quality', 'finish', 'frame', 'strong', 'straight', 'full', 'double', 'bench']\n",
      "Sentiment heavy words:  2/6\n",
      "4'th word's (kill) options: {'kill', 'obliterate', 'hell', 'defeat', 'shit'}\n",
      "24'th word's (solid) options: {'straight', 'clear', 'strong', 'solid', 'substantial'}\n",
      "Number of negations removed : 0\n",
      "--- More options (total: 36) possible ---\n",
      "Actual Sentence:  You are going to kill me but I have not seen DS9. I have been waiting till I can do it in one solid week sitting.  :-0.2382\n",
      "Worst Sentence: You are going to kill me but I have not seen DS9 . I have been waiting till I can do it in one solid week sitting . : -0.2382\n",
      "Best Sentence: You are going to shit me but I have not seen DS9 . I have been waiting till I can do it in one substantial week sitting . : -0.0258\n",
      "Wrote something!\n",
      "\n",
      "\n",
      " 5 Tweet:  i think ur right!! hahaha!! 4.5 hrs now!! :0.6973\n",
      "\n",
      "No antonyms found for word hahaha\n",
      "No synonyms found for word hahaha\n",
      "Some early replacements for word hahaha are ['hahahaha', 'haha', 'hahah', 'hahahah', 'hahahahaha', 'hahahha', 'hahha', 'hahahahahaha']\n",
      "Sentiment heavy words:  1/6\n",
      "6'th word's (hahaha) options: {'haha'}\n",
      "Number of negations removed : 0\n",
      "--- More options (total: 2) possible ---\n",
      "Actual Sentence:  i think ur right!! hahaha!! 4.5 hrs now!!  :0.6973\n",
      "Best Sentence: i think ur right ! ! hahaha ! ! 4.5 hrs now ! ! : 0.6973\n",
      "\n",
      "\n",
      " 6 Tweet: i hate to see the spartans so sad :-0.8267\n",
      "\n",
      "Some synonyms for word hate are ['hatred', 'detest', 'hate']\n",
      "Some early replacements for word hate are ['people', \"n't\", 'stupid', 'why', 'fuck', 'swear', 'really', 'seriously']\n",
      "Some synonyms for word sad are ['pitiful', 'deplorable', 'lamentable']\n",
      "Some early replacements for word sad are ['bad', 'feel', 'sick', 'upset', 'depressed', 'cry', 'really', 'confused']\n",
      "Sentiment heavy words:  2/6\n",
      "1'th word's (hate) options: {'swear', 'hate'}\n",
      "7'th word's (sad) options: {'pitiful', 'bad', 'confused', 'stupid', 'lamentable', 'sick', 'upset', 'depressed'}\n",
      "0'th word's (i) options: {'BOOSTER_FLAG'}\n",
      "Number of negations removed : 0\n",
      "--- More options (total: 216) possible ---\n",
      "Actual Sentence: i hate to see the spartans so sad  :-0.8267\n",
      "Worst Sentence: i overwhelmingly hate to see the spartans so bad : -0.8655\n",
      "Best Sentence: i definitely swear to see the spartans so confused : -0.1258\n",
      "Wrote something!\n",
      "\n",
      "\n",
      " 7 Tweet: My mind and body are severely protesting this quotgetting upquot  thing. Had nightmares to boot :-0.7003\n",
      "\n",
      "No antonyms found for word severely\n",
      "Some synonyms for word severely are ['badly', 'seriously', 'sternly']\n",
      "Some early replacements for word severely are ['severly', 'horribly', 'beaten', 'terribly', 'badly', 'unfairly', 'permanently', 'hugely']\n",
      "No antonyms found for word protesting\n",
      "Some synonyms for word protesting are ['protest', 'resist', 'dissent']\n",
      "Some early replacements for word protesting are ['protest', 'protestors', 'protests', 'protesters', 'activists', 'demonstrators', 'workers', 'rallies']\n",
      "No POS words found for word protesting with tag VBG\n",
      " -- No POS approved words after trimming! -- for word protesting\n",
      " some non-trimmed:['protest', 'protestors', 'protests', 'protesters']\n",
      "Sentiment heavy words:  2/6\n",
      "5'th word's (severely) options: {'badly', 'horribly', 'seriously', 'terribly', 'severely', 'gravely'}\n",
      "Number of negations removed : 0\n",
      "--- More options (total: 7) possible ---\n",
      "Actual Sentence: My mind and body are severely protesting this quotgetting upquot  thing. Had nightmares to boot  :-0.7003\n",
      "Worst Sentence: My mind and body are terribly protesting this quotgetting upquot thing . Had nightmares to boot : -0.7506\n",
      "Best Sentence: My mind and body are seriously protesting this quotgetting upquot thing . Had nightmares to boot : -0.5423\n",
      "Wrote something!\n",
      "\n",
      "\n",
      " 8 Tweet:  I am goin to follow u since u did not  LOL  GO ANGELS!:-0.4882\n",
      "\n",
      " --- LOL not in vocabulary ---\n",
      "Some early replacements for word LOL are []\n",
      "No replacements found at all for word LOL\n",
      "Sentiment heavy words:  1/6\n",
      "9'th word's (not) options: {'not', 'IGNORE_FLAG'}\n",
      "Number of negations removed : 1\n",
      "--- More options (total: 3) possible ---\n",
      "Actual Sentence:  I am goin to follow u since u did not  LOL  GO ANGELS! :-0.4882\n",
      "Best Sentence: I am goin to follow u since u did not LOL GO ANGELS ! : -0.4882\n",
      "\n",
      "\n",
      " 9 Tweet:  I think I want to read some books but the library does not have them :0.0387\n",
      "\n",
      "No antonyms found for word want\n",
      "Some synonyms for word want are ['need', 'wishing', 'deficiency']\n",
      "Some early replacements for word want are ['need', 'if', \"n't\", \"'ll\", 'even', 'wanna', 'give', 'let']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment heavy words:  1/6\n",
      "3'th word's (want) options: {'desire', 'wish', 'want'}\n",
      "2'th word's (I) options: {'BOOSTER_FLAG'}\n",
      "Number of negations removed : 0\n",
      "--- More options (total: 32) possible ---\n",
      "Actual Sentence:  I think I want to read some books but the library does not have them  :0.0387\n",
      "Worst Sentence: I think I want to read some books but the library does not have them : 0.0387\n",
      "Best Sentence: I think I definitely wish to read some books but the library does not have them : 0.4019\n",
      "Wrote something!\n",
      "\n",
      "\n",
      " 10 Tweet: My nap was interrupted so many times today  Going out for Japanese with the  arents again...:-0.296\n",
      "\n",
      "No antonyms found for word interrupted\n",
      "Some synonyms for word interrupted are ['disrupt', 'break', 'cut_off']\n",
      "Some early replacements for word interrupted are ['interrupting', 'rudely', 'interrupts', 'interrupt', 'interupted', 'woken', 'lectured', 'questioned']\n",
      "Sentiment heavy words:  1/6\n",
      "3'th word's (interrupted) options: {'interrupted', 'disturb', 'questioned'}\n",
      "Number of negations removed : 0\n",
      "--- More options (total: 4) possible ---\n",
      "Actual Sentence: My nap was interrupted so many times today  Going out for Japanese with the  arents again... :-0.296\n",
      "Worst Sentence: My nap was disturb so many times today Going out for Japanese with the arents again ... : -0.4019\n",
      "Best Sentence: My nap was questioned so many times today Going out for Japanese with the arents again ... : -0.1027\n",
      "Wrote something!\n",
      "\n",
      "\n",
      " 11 Tweet: Kind of longs for the bus that shows up at the end of Ghost World right now. Ugh. :-0.6249\n",
      "\n",
      " --- Kind not in vocabulary ---\n",
      "Some early replacements for word Kind are []\n",
      "No replacements found at all for word Kind\n",
      " --- Ghost not in vocabulary ---\n",
      "Some early replacements for word Ghost are []\n",
      "No replacements found at all for word Ghost\n",
      " --- Ugh not in vocabulary ---\n",
      "Some early replacements for word Ugh are []\n",
      "No replacements found at all for word Ugh\n",
      "Sentiment heavy words:  3/6\n",
      " -- No new Strings generated ---\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 12 Tweet:  but this is canada  canada is weird. we are supposed to get snow through wednesday. ugh.:-0.6956\n",
      "\n",
      "No antonyms found for word weird\n",
      "Some synonyms for word weird are ['uncanny', 'eldritch', 'weird']\n",
      "Some early replacements for word weird are ['kinda', 'creepy', 'funny', 'awkward', 'strange', 'actually', 'really', 'stupid']\n",
      "No antonyms found for word ugh\n",
      "No synonyms found for word ugh\n",
      "Some early replacements for word ugh are ['ughh', 'fml', 'gosh', 'urgh', 'wtf', 'seriously', 'sigh', 'damn']\n",
      "Sentiment heavy words:  2/6\n",
      "6'th word's (weird) options: {'crazy', 'stupid', 'awkward', 'strange', 'weird'}\n",
      "17'th word's (ugh) options: {'seriously'}\n",
      "Number of negations removed : 0\n",
      "--- More options (total: 12) possible ---\n",
      "Actual Sentence:  but this is canada  canada is weird. we are supposed to get snow through wednesday. ugh. :-0.6956\n",
      "Worst Sentence: but this is canada canada is stupid . we are supposed to get snow through wednesday . ugh . : -0.8519\n",
      "Best Sentence: but this is canada canada is awkward . we are supposed to get snow through wednesday . seriously . : -0.4497\n",
      "Wrote something!\n",
      "\n",
      "\n",
      " 13 Tweet: Awwh babs... you look so sad underneith that shop entrance of quotYesterdays Musikquot  O I like the look of the new transformer movie :-0.3593\n",
      "\n",
      "Some synonyms for word sad are ['pitiful', 'deplorable', 'lamentable']\n",
      "Some early replacements for word sad are ['bad', 'feel', 'sick', 'upset', 'depressed', 'cry', 'really', 'confused']\n",
      "Some synonyms for word like are ['comparable', 'same', 'care']\n",
      "Some early replacements for word like are ['that', 'know', 'think', 'just', 'really', 'it', \"n't\", 'you']\n",
      "Sentiment heavy words:  2/6\n",
      "6'th word's (sad) options: {'pitiful', 'bad', 'confused', 'stupid', 'lamentable', 'sick', 'upset', 'depressed'}\n",
      "16'th word's (like) options: {'wish', 'care', 'like'}\n",
      "5'th word's (so) options: {'BOOSTER_FLAG'}\n",
      "Number of negations removed : 0\n",
      "--- More options (total: 288) possible ---\n",
      "Actual Sentence: Awwh babs... you look so sad underneith that shop entrance of quotYesterdays Musikquot  O I like the look of the new transformer movie  :-0.3593\n",
      "Worst Sentence: Awwh babs ... you look so overwhelmingly bad underneith that shop entrance of quotYesterdays Musikquot O I like the look of the new transformer movie : -0.5056\n",
      "Best Sentence: Awwh babs ... you look so lamentable underneith that shop entrance of quotYesterdays Musikquot O I care the look of the new transformer movie : -0.0107\n",
      "Wrote something!\n",
      "\n",
      "\n",
      " 14 Tweet: sad that the feet of my macbook just fell off :-0.4767\n",
      "\n",
      "Some synonyms for word sad are ['pitiful', 'deplorable', 'lamentable']\n",
      "Some early replacements for word sad are ['bad', 'feel', 'sick', 'upset', 'depressed', 'cry', 'really', 'confused']\n",
      "Sentiment heavy words:  1/6\n",
      "0'th word's (sad) options: {'sad', 'confused', 'BOOSTER_FLAG', 'pitiful', 'sick', 'bad', 'stupid', 'lamentable'}\n",
      "Number of negations removed : 0\n",
      "--- More options (total: 15) possible ---\n",
      "Actual Sentence: sad that the feet of my macbook just fell off  :-0.4767\n",
      "Worst Sentence: sad overwhelmingly that the feet of my macbook just fell off : -0.5574\n",
      "Best Sentence: sad definitely that the feet of my macbook just fell off : -0.1027\n",
      "Wrote something!\n",
      "\n",
      "\n",
      " 15 Tweet: I am gonna get up late tomorrow and it is 132am here. I gonna get tipsy by my lonesome. Thats...thats just sad :-0.6808\n",
      "\n",
      "No antonyms found for word lonesome\n",
      "Some synonyms for word lonesome are ['lonesome', 'only', 'solitary']\n",
      "Some early replacements for word lonesome are ['lonley', 'lonely', 'sheltered', 'crabby', 'secluded', 'darkest', 'weary', 'timenbut']\n",
      "Some synonyms for word sad are ['pitiful', 'deplorable', 'lamentable']\n",
      "Some early replacements for word sad are ['bad', 'feel', 'sick', 'upset', 'depressed', 'cry', 'really', 'confused']\n",
      "Sentiment heavy words:  2/6\n",
      "21'th word's (lonesome) options: {'darkest', 'lonesome', 'lonely', 'lone'}\n",
      "27'th word's (sad) options: {'cry', 'pitiful', 'bad', 'confused', 'stupid', 'sick', 'upset', 'depressed'}\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-50-e2be2cb46668>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m \u001b[0mnumOfTweets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrunThroughTweets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"num of tweets done : {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumOfTweets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-50-e2be2cb46668>\u001b[0m in \u001b[0;36mrunThroughTweets\u001b[0;34m()\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mVERBOSE_PRINTING\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkeysToChange\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"{0}'th word's ({2}) options: {1}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreplacementDictionary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtweetTokens\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0msentenceObj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreturnCombinationsOfStrings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentenceObj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "def extractTwitterDataset():\n",
    "#     df_tweets = pd.read_csv( pathToDatasets + 'cleanedTweets.csv', nrows=NUM_OF_TWEETS, skiprows =TWEET_START)\n",
    "    df_tweets = pd.read_csv( pathToDatasets + \"{0}.csv\".format(DATASET_FILE_NAME), nrows=NUM_OF_TWEETS, skiprows =TWEET_START)\n",
    "    tweets = df_tweets.values\n",
    "    return tweets;\n",
    "\n",
    "def createDictionaryOfSentStrings(sentencesWithSentiment):\n",
    "    dict_sentimentToStringObjects = {}\n",
    "    for obj in sentencesWithSentiment:\n",
    "        sent = obj.getSentiment()\n",
    "        if(dict_sentimentToStringObjects.get(sent) == None):\n",
    "            dict_sentimentToStringObjects[sent] = [obj]\n",
    "        else:\n",
    "            (dict_sentimentToStringObjects[sent]).append(obj)\n",
    "\n",
    "    return dict_sentimentToStringObjects;\n",
    "\n",
    "def runThroughTweets():\n",
    "    tweets = extractTwitterDataset()\n",
    "    \n",
    "    counterOfTweets = 0;\n",
    "    for counter,tweet in enumerate(tweets):\n",
    "        counterOfTweets +=1;\n",
    "        tweet = tweet[0]\n",
    "        tweetTokens = cleanAndTokenizeText(tweet)\n",
    "        mainSentiment = senty.polarity_scores(tweet)['compound']\n",
    "        if(mainSentiment == 0):\n",
    "            continue\n",
    "        print(\"\\n\\n {2} Tweet: {0}:{1}\\n\".format(tweet,mainSentiment, counterOfTweets))   \n",
    "        \n",
    "        sentenceObj = Sentence(tweet, mainSentiment)\n",
    "        sentenceObj = getAlternativeWords(sentenceObj)\n",
    "        sentenceObj = editBoosterWords(sentenceObj)\n",
    "        \n",
    "        replacementDictionary = sentenceObj.getDictOfIndexWords();\n",
    "        if(len(replacementDictionary) <= 0):        \n",
    "            print(\" -- No new Strings generated ---\\n\\n\")\n",
    "            continue\n",
    "\n",
    "        keysToChange = replacementDictionary.keys();\n",
    "        if(VERBOSE_PRINTING):\n",
    "            for key in keysToChange:\n",
    "                if(key not in range(0,len(tweetTokens))): continue\n",
    "                print(\"{0}'th word's ({2}) options: {1}\".format(key,replacementDictionary[key],tweetTokens[key]))\n",
    "\n",
    "        sentenceObj = returnCombinationsOfStrings(sentenceObj)\n",
    "        sentencesWithSentiment, sentenceObj = printStrings(sentenceObj, counterOfTweets)\n",
    "       \n",
    "        dict_sentimentToStringObjects = createDictionaryOfSentStrings(sentencesWithSentiment)\n",
    "        \n",
    "        sentiments = sorted(dict_sentimentToStringObjects.keys())\n",
    "        \n",
    "    return counterOfTweets;\n",
    "        \n",
    "\n",
    "numOfTweets = runThroughTweets()\n",
    "print(\"num of tweets done : {0}\".format(numOfTweets))\n",
    "\n",
    "global LIST_OF_EVAL_OBJECTS\n",
    "\n",
    "with open(\"eval.json\", \"w+\") as evalWriter:\n",
    "        jsonString = json.dumps([obj.__dict__ for obj in LIST_OF_EVAL_OBJECTS])\n",
    "        evalWriter.write(jsonString)\n",
    "print(\"Eval writer written!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "specificWord = \"good\"\n",
    "def testOneWord(word=\"\"):\n",
    "    if(word==\"\"):\n",
    "        return\n",
    "    print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from IPython.core.debugger import set_trace\n",
    "\n",
    "\n",
    "specificString = \"\"\n",
    "def specificString(textString=\"\"):\n",
    "    if(textString == \"\" or textString == None):\n",
    "        return\n",
    "    textString = dataClean(textString)\n",
    "    textTokens = cleanAndTokenizeText(textString)\n",
    "    mainSentiment = senty.polarity_scores(textString)['compound']\n",
    "    if(mainSentiment == 0):\n",
    "        print(\"{} \\n No sentiment found in sentence\".format(textString));\n",
    "        return;\n",
    "    print(\"\\n {0}:{1}\\n\".format(textString,mainSentiment))   \n",
    "    sentenceObj = Sentence(textString, mainSentiment)\n",
    "    sentenceObj = getAlternativeWords(sentenceObj)\n",
    "    sentenceObj = editBoosterWords(sentenceObj)\n",
    "    \n",
    "    \n",
    "    replacementDictionary = sentenceObj.getDictOfIndexWords();\n",
    "    if(len(replacementDictionary) <= 0):        \n",
    "        print(\" -- No new Strings generated ---\\n\\n\")\n",
    "        return\n",
    "    \n",
    "    keysToChange = replacementDictionary.keys();\n",
    "    if(VERBOSE_PRINTING):\n",
    "        for key in keysToChange:\n",
    "               print(\"{0}'th word's ({2}) options: {1}\".format(key,replacementDictionary[key],textTokens[key]))\n",
    "    \n",
    "    sentenceObj = returnCombinationsOfStrings(sentenceObj)\n",
    "    allPossibleSentences, sentenceObj = printStrings(sentenceObj)\n",
    "    \n",
    "print(\"Enter a string to use\")\n",
    "\n",
    "\n",
    "inputText = \"\"\n",
    "inputText = input()\n",
    "if(inputText == \"\"):\n",
    "    return\n",
    "elif(inputText == \"t\"):\n",
    "#     specificString(\"hot chocolate really angers me and it should sadden you too\")\n",
    "#     specificString(\"the grind is inspirational and saddening at the same time. do not want you to stop cuz i like what u do!\")\n",
    "    specificString(\"i hope they will increase the capacity fast yesterday was such a pain. got the fail whale 15 times in 2 hours....\");\n",
    "#     specificString(\"I would love to post the exec summary I raised >$3M with in first round, but its quite frankly a fucking embarrassment. I feel bad for the investors. actually I dont, because well, fuck them. wasnt their money anyway.\")\n",
    "else:\n",
    "    specificString(inputText)\n",
    "# I really love hot chocolate, but I'm not good with hot coffee "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
