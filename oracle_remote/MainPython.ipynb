{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imported clean dataset!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:85% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gensim.downloader as api\n",
    "import numpy as np\n",
    "import re\n",
    "import csv\n",
    "import pandas as pd\n",
    "import pprint\n",
    "import string\n",
    "import nltk\n",
    "import sys\n",
    "import spacy\n",
    "\n",
    "from IPython.display import HTML\n",
    "from nltk.corpus import wordnet \n",
    "from sklearn.manifold import TSNE\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "pathToDatasets = '../datasets/'\n",
    "pathToDataScripts = '../datasets/scripts/'\n",
    "filePath = '../datasets/GoogleNews-vectors-negative300.bin'\n",
    "\n",
    "sys.path.insert(0, pathToDataScripts)\n",
    "from cleanDataset import tokenize_words \n",
    "\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:85% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading binaries and models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Should I reload the model?\n",
      "yes\n",
      "loading the model!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "print(\"Should I reload the model?\")\n",
    "tstString = input()\n",
    "if(\"no\" in tstString.lower()):\n",
    "    print(\" didnt reload model! \")\n",
    "else:\n",
    "    print(\"loading the model!\");\n",
    "    word_vectors = api.load(\"glove-wiki-gigaword-100\")\n",
    "    nltk.download('vader_lexicon')\n",
    "    nltk.download('punkt')\n",
    "    nltk.download('averaged_perceptron_tagger')\n",
    "    nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global Variables and Global Objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "senty = SentimentIntensityAnalyzer()\n",
    "vocabulary = word_vectors.vocab;\n",
    "\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "\n",
    "NUMBER_OF_ALTERNATIVES = 5\n",
    "TWEET_START = 104\n",
    "NUM_OF_TWEETS = 5\n",
    "\n",
    "\n",
    "# VERBOSE_PRINTING = True\n",
    "VERBOSE_PRINTING = False\n",
    "\n",
    "USE_SPACY = False\n",
    "# USE_SPACY = True\n",
    "\n",
    "COLOR_PRINTING = True\n",
    "#COLOR_PRINTING = False\n",
    "\n",
    "\n",
    "\n",
    "punctuation = r\"\\\"#$%&'+-/;<=>?@[\\]*.^_`{|}~\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class for Sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imported sentence class!\n"
     ]
    }
   ],
   "source": [
    "import SentenceClasses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imported SentimentShifting Library\n"
     ]
    }
   ],
   "source": [
    "import ShiftingHelperLibrary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Cell\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imported main shifting functions\n"
     ]
    }
   ],
   "source": [
    "import MainShiftingLibrary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Keep calm and carry on debugging till it works:0.3182\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'Sentence' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-ffa5974b8589>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m# specificString(\"I really hate hot chocolate, but i enjoy hot coffee, and I'm serious\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mspecificString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Keep calm and carry on debugging till it works\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;31m# specificString(\"would rather the first party send bad messages than the 3rd party send mixed ones  sophmore year all over again\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-ffa5974b8589>\u001b[0m in \u001b[0;36mspecificString\u001b[0;34m(textString)\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;32mreturn\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n {0}:{1}\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtextString\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmainSentiment\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0msentenceObj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtextString\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmainSentiment\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0msentenceObj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetAlternativeSentences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentenceObj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0malternateTweets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msentenceObj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malternateSentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Sentence' is not defined"
     ]
    }
   ],
   "source": [
    "specificString = \"\"\n",
    "def specificString(textString=\"\"):\n",
    "    if(textString == \"\" or textString == None):\n",
    "        return\n",
    "    mainSentiment = senty.polarity_scores(textString)['compound']\n",
    "    if(mainSentiment == 0):\n",
    "        print(\"{} \\n No sentiment found in sentence\".format(textString));\n",
    "        return;\n",
    "    print(\"\\n {0}:{1}\\n\".format(textString,mainSentiment))   \n",
    "    sentenceObj = Sentence(textString, mainSentiment)\n",
    "    sentenceObj = getAlternativeSentences(sentenceObj)\n",
    "    alternateTweets = (sentenceObj.alternateSentences)[:]\n",
    "    if(alternateTweets == [] or alternateTweets == None):\n",
    "        print(\" -- No new Strings generated ---\\n\\n\")\n",
    "        return\n",
    "    printStrings(sentenceObj)\n",
    "    printAllPossibleStrings(sentenceObj)\n",
    "\n",
    "# specificString(\"I really hate hot chocolate, but i enjoy hot coffee, and I'm serious\")\n",
    "specificString(\"Keep calm and carry on debugging till it works\");\n",
    "# specificString(\"would rather the first party send bad messages than the 3rd party send mixed ones  sophmore year all over again\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def runThroughTweets():\n",
    "    \n",
    "    tweets_df = pd.read_csv( pathToDatasets + 'cleanedTweets.csv' , nrows=NUM_OF_TWEETS, skiprows=TWEET_START)\n",
    "\n",
    "    tweets = tweets_df.values\n",
    "\n",
    "    listOfObjects = []\n",
    "    for counter,tweet in enumerate(tweets):\n",
    "        tweet = tweet[0]\n",
    "        mainSentiment = senty.polarity_scores(tweet)['compound']\n",
    "        if(mainSentiment == 0):\n",
    "            continue\n",
    "        print(\"\\n {0}:{1}\\n\".format(tweet,mainSentiment))   \n",
    "        sentenceObj = Sentence(tweet, mainSentiment)\n",
    "        sentenceObj = getAlternativeSentences(sentenceObj)\n",
    "        alternateTweets = (sentenceObj.alternateSentences)[:]\n",
    "        if(alternateTweets == [] or alternateTweets == None):\n",
    "            print(\" -- No new Strings generated ---\\n\\n\")\n",
    "            continue\n",
    "        sentenceObj = shiftSentiment(sentenceObj, True);\n",
    "        printStrings(sentenceObj)\n",
    "        sentenceObj = shiftSentiment(sentenceObj, False);\n",
    "        printStrings(sentenceObj)\n",
    "        printAllPossibleStrings(sentenceObj)\n",
    "    \n",
    "runThroughTweets()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "specificWord = \"good\"\n",
    "def testOneWord(word=\"\"):\n",
    "    if(word==\"\"):\n",
    "        return\n",
    "    print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
