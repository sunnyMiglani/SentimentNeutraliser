{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cleanDataset\n",
      "Imported clean dataset!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:85% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gensim.downloader as api\n",
    "import numpy as np\n",
    "import re\n",
    "import csv\n",
    "import pandas as pd\n",
    "import pprint\n",
    "import string\n",
    "import nltk\n",
    "import sys\n",
    "import spacy\n",
    "import os\n",
    "import pickle\n",
    "    \n",
    "from IPython.display import HTML\n",
    "from nltk.corpus import wordnet \n",
    "from sklearn.manifold import TSNE\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "pathToDatasets = '../datasets/'\n",
    "pathToDataScripts = '../datasets/scripts/'\n",
    "filePath = '../datasets/GoogleNews-vectors-negative300.bin'\n",
    "# modelBeingUsed = \"glove-wiki-gigaword-300\"\n",
    "# modelBeingUsed = \"glove-wiki-gigaword-100\"\n",
    "# modelBeingUsed = \"glove-twitter-50\"\n",
    "# modelBeingUsed = \"glove-twitter-100\"\n",
    "modelBeingUsed = \"glove-twitter-200\"\n",
    "\n",
    "sys.path.insert(0, pathToDataScripts)\n",
    "from cleanDataset import tokenize_words, dataClean\n",
    "\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:85% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading binaries and models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Should I reload the model?\n",
      "\n",
      "loading glove-twitter-200!\n",
      "loading via pickle!\n",
      "Model Loaded!\n",
      "{'definitely', '', 'absolutely', 'completely', 'especially'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "print(\"Should I reload the model?\")\n",
    "tstString = input()\n",
    "if(\"no\" in tstString.lower() or \"n\" in tstString.lower()):\n",
    "    print(\" didnt reload model! \")\n",
    "else:\n",
    "    print(\"loading {0}!\".format(modelBeingUsed));\n",
    "    fileName = \"{}.pickle\".format(modelBeingUsed)\n",
    "    if(os.path.exists(pathToDatasets+fileName)):\n",
    "        print(\"loading via pickle!\")\n",
    "        pickle_in = open(pathToDatasets+fileName, \"rb\")\n",
    "        word_vectors = pickle.load(pickle_in);\n",
    "    else:\n",
    "        print(\"Pickle didn't exist, therefore loading model!\")\n",
    "        word_vectors = api.load(\"{0}\".format(modelBeingUsed))\n",
    "        print(\"-- Saving to pickle file for next time! --\")\n",
    "        pickle_out = open(pathToDatasets+fileName,\"wb\")\n",
    "        pickle.dump(word_vectors, pickle_out)\n",
    "        pickle_out.close()\n",
    "        \n",
    "    nltk.download('vader_lexicon')\n",
    "    nltk.download('punkt')\n",
    "    nltk.download('averaged_perceptron_tagger')\n",
    "    nltk.download('wordnet')\n",
    "    print(\"Model Loaded!\")\n",
    "\n",
    "setOfBoosterWords = set()\n",
    "with open(pathToDatasets + \"BoosterWordList.txt\") as bwf:\n",
    "    pd_booster = pd.read_csv(bwf, sep='\\t');\n",
    "    listOfBoosterWords = (pd_booster.iloc[:,0]).tolist() \n",
    "    setOfBoosterWords = set(listOfBoosterWords[:4])\n",
    "setOfBoosterWords.add(\"\") # So we have a case where we're not adding any boosters.\n",
    "print(setOfBoosterWords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global Variables and Global Objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "senty = SentimentIntensityAnalyzer()\n",
    "vocabulary = word_vectors.vocab;\n",
    "\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "\n",
    "'''\n",
    "Good sets -> {100+20}\n",
    "'''\n",
    "TWEET_START = 100\n",
    "NUM_OF_TWEETS = 5\n",
    "\n",
    "VERBOSE_PRINTING = True\n",
    "# VERBOSE_PRINTING = False\n",
    "\n",
    "# USE_SPACY = False\n",
    "USE_SPACY = True\n",
    "\n",
    "COLOR_PRINTING = True\n",
    "# COLOR_PRINTING = False\n",
    "\n",
    "# PRINT_NEUTRAL = True\n",
    "PRINT_NEUTRAL = False\n",
    "\n",
    "# PRINT_ALL_STRINGS = True\n",
    "PRINT_ALL_STRINGS = False\n",
    "\n",
    "HANDLE_NEGATIONS = True\n",
    "# HANDLE_NEGATIONS = False\n",
    "\n",
    "HANDLE_BOOSTERS = True\n",
    "# HANDLE_BOOSTERS = False\n",
    "\n",
    "\n",
    "SHOW_ALTS = 50\n",
    "\n",
    "NUMBER_OF_NEAR = 6\n",
    "MAX_SENTIMENT_HEAVY_WORDS = 3\n",
    "MAX_REPLACEMENTS = 7\n",
    "\n",
    "punctuation = r\"\\\"#$%&'+-/;<=>?@[\\]*^_`{|}~\"\n",
    "LIST_OF_NEGATIONS = [\"not\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class for Sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imported class file!\n"
     ]
    }
   ],
   "source": [
    "from SentenceClass import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printStrings(sentenceObj):\n",
    "    \n",
    "    numberOfPrints = 0\n",
    "    newStrings = generateHTMLObjectsFromSentence(sentenceObj)\n",
    "    mainSentiment = sentenceObj.ogSentiment;\n",
    "    listOfSentencesWithSentiments = []\n",
    "    bestSentiment = -sys.maxsize - 1\n",
    "    worstSentiment = sys.maxsize\n",
    "    \n",
    "    isPositiveSentence = True if(mainSentiment >= 0) else False\n",
    "    \n",
    "    bestSentimentString = \"\";\n",
    "    worstSentimentString = \"\";\n",
    "    \n",
    "    numberOfSentimentHeavyWordsYet = 0;\n",
    "    for ind, tSentence in enumerate(newStrings):\n",
    "            alteredTweet = tSentence.getSentence()\n",
    "            htmlText = tSentence.getHTML()\n",
    "            numberOfPrints+=1;\n",
    "            \n",
    "            \n",
    "            sentimentOfNewString = senty.polarity_scores(alteredTweet)['compound']\n",
    "            \n",
    "            thisStringPositive = True if(sentimentOfNewString >= 0) else False\n",
    "            if(thisStringPositive != isPositiveSentence):\n",
    "                continue\n",
    "            \n",
    "            newObj = SentenceWithSentiment(alteredTweet, sentimentOfNewString, htmlText)\n",
    "            listOfSentencesWithSentiments.append(newObj)\n",
    "            \n",
    "            if(sentimentOfNewString >= bestSentiment):\n",
    "                bestSentiment = sentimentOfNewString\n",
    "                bestSentimentString = htmlText;\n",
    "            elif(sentimentOfNewString < worstSentiment):\n",
    "                worstSentiment = sentimentOfNewString\n",
    "                worstSentimentString = htmlText;\n",
    "            \n",
    "            \n",
    "#             if(numberOfPrints > SHOW_ALTS): break\n",
    "            if(numberOfPrints > SHOW_ALTS or PRINT_ALL_STRINGS == False): continue\n",
    "            if(sentimentOfNewString == mainSentiment or sentimentOfNewString == 0.0):\n",
    "                if(PRINT_ALL_STRINGS and numberOfPrints <= SHOW_ALTS): displayText(\"{0}: {1}\".format(htmlText,sentimentOfNewString),'black')\n",
    "            elif(sentimentOfNewString > mainSentiment):\n",
    "                if(PRINT_ALL_STRINGS and numberOfPrints <= SHOW_ALTS): displayText(\"{0}: {1}\".format(htmlText,sentimentOfNewString),'green')\n",
    "            elif(sentimentOfNewString < mainSentiment and sentimentOfNewString != 0.0):\n",
    "                if(PRINT_ALL_STRINGS and numberOfPrints <= SHOW_ALTS): displayText(\"{0}: {1}\".format(htmlText,sentimentOfNewString),'red')\n",
    "                \n",
    "#     if(numberOfPrints > SHOW_ALTS): print(\"--- More options (total: {0}) possible, but not printed ---\".format(numberOfPrints));\n",
    "    print(\"--- More options (total: {0}) possible ---\".format(numberOfPrints));\n",
    "    displayText(\"Actual Sentence: {0} :{1}\".format(sentenceObj.ogSentence, sentenceObj.ogSentiment))\n",
    "    if (worstSentimentString != \"\"): displayText(\"Worst Sentence: {0} : {1}\".format(worstSentimentString, worstSentiment), color='red')\n",
    "    if (bestSentimentString != \"\"): displayText(\"Best Sentence: {0} : {1}\".format(bestSentimentString, bestSentiment), color='green') \n",
    "    \n",
    "   \n",
    "    sentenceObj.addFinalSentences(listOfSentencesWithSentiments)\n",
    "    return listOfSentencesWithSentiments, sentenceObj;\n",
    "\n",
    "def cstr(s, color='black', italics=False):\n",
    "    if(COLOR_PRINTING):\n",
    "        if(italics):\n",
    "            return cstr(\"<i>{0}</i>\".format(s), color);\n",
    "        return \"<text style=color:{}>{}</text>\".format(color, s)\n",
    "    else:\n",
    "        return \"{}\".format(s)\n",
    "\n",
    "def displayText(text, color='black'):\n",
    "    if(COLOR_PRINTING):\n",
    "        display(HTML(cstr(text, color)));\n",
    "        return\n",
    "    print(\"{}\".format(text));\n",
    "    \n",
    "    \n",
    "def cleanAndTokenizeText(text):\n",
    "    text = text.lower();\n",
    "    newString = \"\"\n",
    "    for char in text:\n",
    "        if char not in punctuation:\n",
    "            newString += char\n",
    "    text = word_tokenize(newString)\n",
    "    return text;\n",
    "\n",
    "def getPOSTags(tweet):\n",
    "    if(USE_SPACY == False):\n",
    "        tags = nltk.pos_tag(tweet)\n",
    "        return tags;    \n",
    "    tweet = ' '.join(tweet)\n",
    "    doc = nlp(tweet)\n",
    "    tags = [(token.text, token.tag_) for token in doc] # since the format expected is [text,tag]\n",
    "    return tags\n",
    "\n",
    "def getAntonymsAndSynonymsOfWords(word):\n",
    "    if(word not in vocabulary):\n",
    "        return [], []\n",
    "    setOfAntonyms = set()\n",
    "    setOfSynonyms = set()\n",
    "    for syn in wordnet.synsets(word):\n",
    "        for l in syn.lemmas():\n",
    "            setOfSynonyms.add(l.name().lower())\n",
    "            anton = l.antonyms()\n",
    "            if(anton!=[]):\n",
    "                setOfAntonyms.add(anton[0].name().lower())\n",
    "    if(len(setOfAntonyms) == 0):\n",
    "        if(VERBOSE_PRINTING): print(\"No antonyms found for word {0}\".format(word))\n",
    "    if(len(setOfSynonyms) == 0):\n",
    "        if(VERBOSE_PRINTING): print(\"No synonyms found for word {0}\".format(word))\n",
    "            \n",
    "    return [], list(setOfSynonyms)\n",
    "#     return list(setOfAntonyms), list(setOfSynonyms)\n",
    "\n",
    "\n",
    "def returnReplacementsForWord(word):\n",
    "    \n",
    "    if(word not in vocabulary):\n",
    "        print(\" --- {0} not in vocabulary ---\".format(word))\n",
    "        return []\n",
    "    possibleReplacements = [word[0] for word in word_vectors.most_similar(word,topn=NUMBER_OF_NEAR)]\n",
    "    \n",
    "    if(possibleReplacements == []):\n",
    "        print(\" --- No replacements for word {0} ---\".format(word))\n",
    "    antonyms,synonyms = getAntonymsAndSynonymsOfWords(word)\n",
    "    \n",
    "    if(antonyms != []):\n",
    "        possibleReplacements.extend(antonyms)\n",
    "        if(VERBOSE_PRINTING): print(\"Some antonyms for word {0} are {1}\".format(word, antonyms[:3]))\n",
    "    if(synonyms != []):\n",
    "        possibleReplacements.extend(synonyms)\n",
    "        if(VERBOSE_PRINTING): print(\"Some synonyms for word {0} are {1}\".format(word, synonyms[:3]))\n",
    "    return possibleReplacements\n",
    "    \n",
    "def posApprovedReplacements(alternativeWords, userTokens, indexOfToken):\n",
    "    if(alternativeWords == []):\n",
    "        return []\n",
    "    tempTokens = userTokens[:]\n",
    "    POSTokens = getPOSTags(tempTokens)\n",
    "    validWords = []\n",
    "    \n",
    "    mainTag = POSTokens[indexOfToken][1]\n",
    "    mainWord = userTokens[indexOfToken]\n",
    "    \n",
    "    for ind,word in enumerate(alternativeWords):\n",
    "        if(\"_\" in word): continue\n",
    "        tempTokens[indexOfToken] = word\n",
    "        posTags = getPOSTags(tempTokens)\n",
    "        newTag = (posTags[indexOfToken])[1]\n",
    "        if(str(newTag) == str(mainTag)):\n",
    "#             if(VERBOSE_PRINTING): print(\"Word {0}[{1}] matched with {2}[{3}]\".format(mainWord, mainTag, word,newTag))\n",
    "            validWords.append(word)\n",
    "    if(validWords == [] and VERBOSE_PRINTING):\n",
    "        print(\"No POS words found for word {} with tag {}\".format(mainWord, mainTag));\n",
    "    return validWords\n",
    "    \n",
    "def editBoosterWords(sentenceObj):\n",
    "    if(HANDLE_BOOSTERS == False): return sentenceObj;\n",
    "    ogSentence = sentenceObj.ogSentence;\n",
    "    \n",
    "    tokenizedSentence = cleanAndTokenizeText(ogSentence)\n",
    "    \n",
    "    '''\n",
    "        Use the indexToAlternatives dictionary to add words into the sentences, as part of the chunking process \n",
    "        The problem with this instinctively is how do you have an index \"between\" two variables \n",
    "        You can't really. You'd have to have a flag that says which places it's meant to go \n",
    "        You could do it _before_ the sentiment heavy word (this makes more natural sense) or \n",
    "            you could do it _after_ the word before the sentiment heavy word (this could have its own advantages) \n",
    "    '''\n",
    "    \n",
    "    posTagsForWords = getPOSTags(tokenizedSentence)    \n",
    "    for ind,(word,tag) in enumerate(posTagsForWords):\n",
    "        tmp = ' '.join(tokenizedSentence[ind:ind+2])    \n",
    "        if(word in setOfBoosterWords):\n",
    "            if(senty.polarity_scores(tmp)['compound'] != 0):\n",
    "                sentenceObj.addAlternativesByIndex(ind, [\"IGNORE_FLAG\", word]);\n",
    "                continue\n",
    "        elif(senty.polarity_scores(tmp)['compound'] != 0 and (\"RB\" in tag or \"JJ\" in tag or \"VB\" in tag)):  # \"JJ\" in tag and\n",
    "            newInd = ind-1 if ((ind-1) >= 0 ) else 0\n",
    "            sentenceObj.addAlternativesByIndex(newInd, [\"BOOSTER_FLAG\"]);\n",
    "            if(VERBOSE_PRINTING): print(\"Placed booster at {0} from word {1} on word {2}\".format(newInd, word,tokenizedSentence[newInd] ))\n",
    "    \n",
    "    return sentenceObj\n",
    "\n",
    "\n",
    "def generateHTMLObjectsFromSentence(sentenceObj):\n",
    "    \n",
    "    allSentences = sentenceObj.getFinalSentences()\n",
    "    indexToAlts = sentenceObj.indexToSetOfWords;\n",
    "    indexToChange = list(indexToAlts.keys());\n",
    "    \n",
    "    numberOfNegationsRemoved = 0;\n",
    "    listOfSentenceObjs = []\n",
    "    for t_sentenceObj in allSentences:\n",
    "        sentence = t_sentenceObj.getSentence()\n",
    "        sentence = t_sentenceObj.getSentence()\n",
    "        copySentence = cleanAndTokenizeText(sentence)\n",
    "        subIndex = 0\n",
    "        for index in sorted(indexToChange):\n",
    "            if(index > len(copySentence)):  \n",
    "                copySentence[len(copySentence) - index] = cstr(\"[{0}]\".format(copySentence[len(copySentence) - index]), \"blue\", italics=True);\n",
    "                continue\n",
    "            else:\n",
    "                if(\"IGNORE_FLAG\" in indexToAlts[index]):\n",
    "                    if(copySentence[index] in LIST_OF_NEGATIONS):\n",
    "                        continue; ## Word that _should_ be removed is present.\n",
    "                    else:\n",
    "                        subIndex +=1\n",
    "                        numberOfNegationsRemoved +=1;\n",
    "                        continue;\n",
    "                        \n",
    "                if(\"BOOSTER_FLAG\" in indexToAlts[index]):\n",
    "                    copySentence[index] = cstr(\"[{0}]\".format(copySentence[index]), \"blue\", italics=True);\n",
    "#                     copySentence[index+2] = cstr(\"[{0}]\".format(copySentence[index+2]), \"blue\", italics=True);\n",
    "                    continue\n",
    "                    \n",
    "            copySentence[index-subIndex] = cstr(\"[{0}]\".format(copySentence[index-subIndex]), \"blue\", italics=True);\n",
    "        listOfSentenceObjs.append(SentenceWithHTML(sentence, ' '.join(copySentence)));\n",
    "    \n",
    "    if(VERBOSE_PRINTING): print(\"Number of negations removed : {0}\".format(numberOfNegationsRemoved))\n",
    "    return listOfSentenceObjs\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence Chunking and Appending"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def helper_combine(mainList, myList):\n",
    "    '''\n",
    "    helper function for CombineSentenceChunks\n",
    "    '''\n",
    "    newList = []\n",
    "    for val in myList:\n",
    "        for mainVal in mainList:\n",
    "#             if(VERBOSE_PRINTING): print(\"Combining {0} with {1}\".format(' '.join(val), ' '.join(mainVal)));\n",
    "            newList.append(val + mainVal);\n",
    "    return newList;\n",
    "\n",
    "def combineSentenceChunks(wholeSentence, dictOfChunks):\n",
    "    '''\n",
    "        Uses the helper_combine function to generate all possible combinatios and permuations of the chunks\n",
    "        and any alternatives.\n",
    "        \n",
    "        The logic is to use the end of the sentence, apply each possible chunk from the previous key's chunks\n",
    "        to every possible chunk of this key's.\n",
    "        \n",
    "        The helper function is used to allow us to reuse the list of alreadyGeneratedChunks and constantly\n",
    "        append to them.\n",
    "        \n",
    "        To understand the logic better, take a look at this gist:\n",
    "        https://gist.github.com/sunnyMiglani/cf85407a9e6928237b1436cc2bc95fa4\n",
    "        \n",
    "    '''\n",
    "    reversedKeys = sorted(dictOfChunks.keys(), reverse=True)\n",
    "    completeSentences = [];\n",
    "    mainArr = dictOfChunks[reversedKeys[0]]\n",
    "    for ind in range(1, len(reversedKeys)):\n",
    "        key = reversedKeys[ind];\n",
    "        mainArr = helper_combine(mainArr, dictOfChunks[key]);\n",
    "        \n",
    "    return mainArr;\n",
    "        \n",
    "def generateSentenceChunks(wholeSentence, keyToChange, nextKey, listOfMyAlternatives):\n",
    "    '''\n",
    "        Generates sentence chunks by iterating through the list of alternatives\n",
    "        Chunking the sentence to start from current key to next key.\n",
    "        This means that the sentence always goes from key 'x' to key 'y'\n",
    "        \n",
    "        Example:\n",
    "        \"I really <hate> hot chocolate, but I <prefer> hot coffee\"\n",
    "        Calling generateSentenceChunks will create an example sentence:\n",
    "            - \"<altWordForHate> hot chocolate , but I \"\n",
    "        \n",
    "        Remember to append the first stretch of the string to the first key's chunk for proper use!\n",
    "    '''\n",
    "    \n",
    "    newList = list(listOfMyAlternatives)\n",
    "    newList.append(wholeSentence[keyToChange]);\n",
    "    generatedSentences = []\n",
    "    \n",
    "        \n",
    "    for myAlt in newList:\n",
    "        if(\"IGNORE_FLAG\" == myAlt):\n",
    "            newSentence = wholeSentence[keyToChange+1:nextKey];\n",
    "            generatedSentences.append(newSentence)\n",
    "            continue\n",
    "        elif(\"BOOSTER_FLAG\" == myAlt): \n",
    "            for booster in setOfBoosterWords:\n",
    "                newSentence = wholeSentence[:]\n",
    "                newSentence.insert(keyToChange+1, booster)\n",
    "                generatedSentences.append(newSentence[keyToChange:nextKey+1]);\n",
    "        else:\n",
    "            newSentence = wholeSentence[:]\n",
    "            newSentence[keyToChange] = myAlt\n",
    "            generatedSentences.append(newSentence[keyToChange:nextKey]);\n",
    "        \n",
    "    return generatedSentences\n",
    "    \n",
    "def returnCombinationsOfStrings(sentenceObj):\n",
    "    \n",
    "    indexToWordDict = sentenceObj.indexToSetOfWords;\n",
    "    originalSentence = sentenceObj.ogSentence;\n",
    "    tokenizedSentence = cleanAndTokenizeText(originalSentence)\n",
    "    reversedKeys = sorted(indexToWordDict.keys(), reverse=True)\n",
    "    dictAlternatives  = {}\n",
    "\n",
    "    sortedKeys = sorted(indexToWordDict.keys())\n",
    "    sentenceChunks = {}\n",
    "    htmlChunks = {}\n",
    "    \n",
    "    for ind in range(0,len(sortedKeys)):\n",
    "        key = sortedKeys[ind]\n",
    "        nextKey = sortedKeys[ind+1] if ind+1 < len(sortedKeys) else len(tokenizedSentence)\n",
    "        sentenceChunks[key] = generateSentenceChunks(tokenizedSentence, key, nextKey, indexToWordDict[key])\n",
    "\n",
    "    if(sortedKeys[0] != 0):\n",
    "        newList = []\n",
    "        for thislist in sentenceChunks[sortedKeys[0]]:\n",
    "            newList.append(tokenizedSentence[0:sortedKeys[0]] + thislist)\n",
    "        sentenceChunks[sortedKeys[0]] = newList;\n",
    "        \n",
    "    finalOptions = combineSentenceChunks(tokenizedSentence, sentenceChunks)\n",
    "    \n",
    "    finalSentences = []\n",
    "    for val in finalOptions:\n",
    "        sentence = ' '.join(val)\n",
    "        sentiment = senty.polarity_scores(sentence)['compound']\n",
    "        finalSentences.append(SentenceWithSentiment(sentence,sentiment))\n",
    "    \n",
    "    sentenceObj.addFinalSentences(finalSentences)   \n",
    "    return sentenceObj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trimSuggestions(sentenceObj, mainWord, listOfAlternatives):\n",
    "    mainSentiment = senty.polarity_scores(mainWord)['compound']\n",
    "    isPositive = True if(mainSentiment > 0) else False\n",
    "    finalWords = [mainWord]\n",
    "    for word in listOfAlternatives:\n",
    "        senty_suggestion = senty.polarity_scores(word)['compound'];\n",
    "        isPositive_suggestion = True if senty_suggestion > 0 else False\n",
    "        if(isPositive_suggestion != isPositive or senty_suggestion == 0):\n",
    "            continue\n",
    "        else:\n",
    "            finalWords.append(word)\n",
    "    return finalWords\n",
    "\n",
    "def getAlternativeWords(sentenceObj):\n",
    "    mainSentence = sentenceObj.ogSentence;\n",
    "    mainSentiment = sentenceObj.ogSentiment;\n",
    "\n",
    "    sentenceTokens = cleanAndTokenizeText(mainSentence)\n",
    "\n",
    "    numberOfSentimentHeavyWords = 0;\n",
    "    for ind, word in enumerate(sentenceTokens):\n",
    "        alternativeSentenceWithHTML = []\n",
    "        copyOfTokens = sentenceTokens[:]\n",
    "        replacements = []\n",
    "        replacements = sentenceObj.checkIfWordExists(word)\n",
    "        if(replacements != []):\n",
    "            print(\"FOUND REPEATED WORD {0}\".format(word))\n",
    "            sentenceObj.addAlternativesByIndex(ind, replacements)\n",
    "            continue;\n",
    "        \n",
    "        if(HANDLE_NEGATIONS):\n",
    "            if(word in LIST_OF_NEGATIONS):\n",
    "                '''\n",
    "                    If the word is in the list of negations\n",
    "                    We add a [IGNORE_FLAG] tag to the alternatives\n",
    "                    Which will be grabbed in the future by the sentence generation algorithm. (chunking)\n",
    "                '''\n",
    "                tmp = ' '.join(sentenceTokens[ind:ind+3])\n",
    "                if(senty.polarity_scores(tmp)['compound'] != 0):\n",
    "                    sentenceObj.addAlternativesByIndex(ind, [\"IGNORE_FLAG\", word]);\n",
    "                    continue\n",
    "\n",
    "        score = senty.polarity_scores(word)['compound']\n",
    "        if(score != 0.0):\n",
    "            if(numberOfSentimentHeavyWords < MAX_SENTIMENT_HEAVY_WORDS):\n",
    "                numberOfSentimentHeavyWords+=1;\n",
    "            else:\n",
    "                print(\"Sentiment heavy words:  {0}/{1}\".format(numberOfSentimentHeavyWords, MAX_SENTIMENT_HEAVY_WORDS))\n",
    "                return sentenceObj\n",
    "            \n",
    "            tempReplacements = returnReplacementsForWord(word) # get embedding based relations\n",
    "            \n",
    "            if(VERBOSE_PRINTING): print(\"Some early replacements for word [{0}] are {1}\".format(word, tempReplacements[0:8]))\n",
    "            if(tempReplacements == []):\n",
    "                print(\"No replacements found at all for word {0}\".format(word))\n",
    "                continue\n",
    "            \n",
    "            replacements = posApprovedReplacements(tempReplacements[:], copyOfTokens[:], ind)\n",
    "            finalReplacements = trimSuggestions(sentenceObj,word, replacements)\n",
    "            \n",
    "            if(finalReplacements == []):\n",
    "                if(VERBOSE_PRINTING): print(\" -- No POS approved words after trimming! -- for word {0}\\n some non-trimmed:{1}\".format(word, tempReplacements[:4]))\n",
    "                continue\n",
    "            \n",
    "            # Trimming until max replacements to ensure efficiency\n",
    "            finalReplacements = finalReplacements[:MAX_REPLACEMENTS]\n",
    "            sentenceObj.addAlternativesByIndex(ind, finalReplacements)\n",
    "            sentenceObj.addWordToAlternatives(word, finalReplacements)\n",
    "    print(\"Sentiment heavy words:  {0}/{1}\".format(numberOfSentimentHeavyWords, MAX_SENTIMENT_HEAVY_WORDS))\n",
    "    return sentenceObj\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interactionHelper(dictOfAltObjs, sentiments, thisSent, lastWorkingIndex=0):\n",
    "    if(thisSent in sentiments):\n",
    "        mainIndex = sentiments.index(thisSent)\n",
    "        \n",
    "    else:\n",
    "        mainIndex = lastWorkingIndex\n",
    "    \n",
    "    numLower =  mainIndex  # Mod to keep it in range 0 -> val \n",
    "    numHigher = len(sentiments) - mainIndex - 1\n",
    "\n",
    "    print(\"There are totally {0} possible sentences\".format(len(sentiments)))\n",
    "    print(\"Current sentiment is at index {0}, and length of sentiments is {1}\".format(mainIndex, len(sentiments)));\n",
    "    print(\" <--- {0} lower and {1} ---> higher \".format(numLower, numHigher))\n",
    "    print(\"Would you like to see a lower sentiment or higher sentiment or ALL?\")\n",
    "    print(\"Please enter 1 for lower, 2 for higher or 0 for all\")\n",
    "    \n",
    "    targetSentiment = thisSent\n",
    "    textInput = input()\n",
    "    if(textInput == \"\"):\n",
    "        print(\"You entered nothing! exiting\")\n",
    "        return\n",
    "        \n",
    "    if(textInput.isdigit()):\n",
    "        textInput = int(textInput)\n",
    "        if(textInput == 0):\n",
    "            if(VERBOSE_PRINTING): print(\"Printing sentiments from : {0}\".format(sentiments))\n",
    "            for sentiment in sentiments:\n",
    "                obj = dictOfAltObjs[sentiment][0];\n",
    "                displayText(\"{0}  : {1}\".format(obj.getHTML(), sentiment))\n",
    "            return\n",
    "        elif(textInput == 1 and numLower > 0 and (mainIndex - 1) >= 0):\n",
    "            targetSentiment = sentiments[mainIndex - 1]\n",
    "            htmlSentence = dictOfAltObjs[targetSentiment][0].getHTML();\n",
    "            displayText(\"{0} : {1}\".format(htmlSentence, targetSentiment))\n",
    "            \n",
    "        elif(textInput == 2 and numHigher < len(sentiments) and (mainIndex + 1) < len(sentiments)):\n",
    "            targetSentiment = sentiments[mainIndex + 1]\n",
    "            htmlSentence = dictOfAltObjs[targetSentiment][0].getHTML();\n",
    "            displayText(\"{0} : {1}\".format(htmlSentence, targetSentiment))\n",
    "            \n",
    "        else:\n",
    "            print(\"You entered an invalid option\")\n",
    "            interactionHelper(dictOfAltObjs, sentiments, thisSent, mainIndex)\n",
    "            return\n",
    "        \n",
    "    interactionHelper(dictOfAltObjs, sentiments, targetSentiment,mainIndex)\n",
    "\n",
    "\n",
    "\n",
    "def interactionWithUser(mainSentenceObj, dictOfAltObjs,sentiments, isUser=False):\n",
    "    if(isUser == False):\n",
    "        return\n",
    "    else:\n",
    "        mainSentence = mainSentenceObj.ogSentence;\n",
    "        mainSent = mainSentenceObj.ogSentiment;\n",
    "        if(VERBOSE_PRINTING):\n",
    "            print(\"The following are the sentiment values for the input\")\n",
    "            for ind,sent in enumerate(sentiments):\n",
    "                print(\"{0} - {1}\".format(ind+1,sent))\n",
    "        \n",
    "        interactionHelper(dictOfAltObjs, sentiments, mainSent) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " Tweet:  body of missing northern calif. girl found police have found the remains of a missing northern california girl .. :-0.5267\n",
      "\n",
      "Some synonyms for word missing are ['lacking', 'overlook', 'neglect']\n",
      "Some early replacements for word [missing] are ['missed', 'lost', 'found', 'been', 'missin', 'seeing', 'lacking', 'overlook']\n",
      "FOUND REPEATED WORD missing\n",
      "Sentiment heavy words:  1/3\n",
      "Placed booster at 1 from word missing on word of\n",
      "2'th word's (missing) options: {'missing'}\n",
      "14'th word's (missing) options: {'missing'}\n",
      "1'th word's (of) options: {'BOOSTER_FLAG'}\n",
      "Number of negations removed : 0\n",
      "--- More options (total: 24) possible ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<text style=color:black>Actual Sentence:  body of missing northern calif. girl found police have found the remains of a missing northern california girl ..  :-0.5267</text>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<text style=color:red>Worst Sentence: body <text style=color:blue><i>[of]</i></text> <text style=color:blue><i>[absolutely]</i></text> missing northern calif. girl found police have found the remains of <text style=color:blue><i>[a]</i></text> missing northern california girl .. : -0.6106</text>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<text style=color:green>Best Sentence: body <text style=color:blue><i>[of]</i></text> <text style=color:blue><i>[definitely]</i></text> missing northern calif. girl found police have found the remains of <text style=color:blue><i>[a]</i></text> missing northern california girl .. : -0.1779</text>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " Tweet:  i hope they will increase the capacity fast yesterday was such a pain. got the fail whale 15 times in 2 hours.... :-0.3818\n",
      "\n",
      "Some synonyms for word hope are ['leslie_townes_hope', 'go_for', 'bob_hope']\n",
      "Some early replacements for word [hope] are ['hopefully', 'well', 'hoping', 'everyone', \"'ll\", 'sure', 'leslie_townes_hope', 'go_for']\n",
      "Some synonyms for word increase are ['gain', 'step-up', 'addition']\n",
      "Some early replacements for word [increase] are ['increased', 'decrease', 'increasing', 'increases', 'improve', 'boost', 'gain', 'step-up']\n",
      "SENTY DIDN'T MATCH!\n",
      "SENTY DIDN'T MATCH!\n",
      "SENTY DIDN'T MATCH!\n",
      "SENTY DIDN'T MATCH!\n",
      "Some synonyms for word pain are ['pain_sensation', 'infliction', 'painfulness']\n",
      "Some early replacements for word [pain] are ['hurts', 'pains', 'hurt', 'stress', 'sadness', 'suffering', 'pain_sensation', 'infliction']\n",
      "Sentiment heavy words:  3/3\n",
      "Placed booster at 0 from word hope on word i\n",
      "Placed booster at 3 from word increase on word will\n",
      "Placed booster at 15 from word fail on word the\n",
      "1'th word's (hope) options: {'hope', 'trust', 'desire', 'promise'}\n",
      "4'th word's (increase) options: {'gain', 'growth', 'increase', 'improve', 'boost'}\n",
      "12'th word's (pain) options: {'hurt', 'painfulness', 'sadness', 'stress', 'pain', 'suffering'}\n",
      "0'th word's (i) options: {'BOOSTER_FLAG'}\n",
      "3'th word's (will) options: {'BOOSTER_FLAG'}\n",
      "15'th word's (the) options: {'BOOSTER_FLAG'}\n",
      "Number of negations removed : 0\n",
      "--- More options (total: 45360) possible ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<text style=color:black>Actual Sentence:  i hope they will increase the capacity fast yesterday was such a pain. got the fail whale 15 times in 2 hours....  :-0.3818</text>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<text style=color:red>Worst Sentence: <text style=color:blue><i>[i]</i></text> <text style=color:blue><i>[promise]</i></text> they <text style=color:blue><i>[will]</i></text> <text style=color:blue><i>[increase]</i></text> the capacity fast yesterday was such a <text style=color:blue><i>[painfulness]</i></text> . got <text style=color:blue><i>[the]</i></text> absolutely fail whale 15 times in 2 hours ... . : -0.5984</text>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<text style=color:green>Best Sentence: <text style=color:blue><i>[i]</i></text> <text style=color:blue><i>[hope]</i></text> they <text style=color:blue><i>[will]</i></text> <text style=color:blue><i>[especially]</i></text> increase the capacity fast yesterday was such <text style=color:blue><i>[a]</i></text> painfulness . <text style=color:blue><i>[got]</i></text> the definitely fail whale 15 times in 2 hours ... . : -0.0018</text>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " Tweet:  remember my bum leg strikes back this time its serious :-0.4215\n",
      "\n",
      "Some synonyms for word strikes are ['shine', 'coin', 'tap']\n",
      "Some early replacements for word [strikes] are ['strike', 'struck', 'hits', 'attacks', 'kills', 'threatens', 'shine', 'coin']\n",
      "Some synonyms for word serious are ['dangerous', 'grave', 'life-threatening']\n",
      "Some early replacements for word [serious] are ['seriously', 'really', 'but', 'still', 'funny', 'talk', 'dangerous', 'grave']\n",
      "SENTY DIDN'T MATCH!\n",
      "Sentiment heavy words:  2/3\n",
      "Placed booster at 3 from word strikes on word leg\n",
      "Placed booster at 8 from word serious on word its\n",
      "4'th word's (strikes) options: {'threatens', 'strikes', 'kills'}\n",
      "9'th word's (serious) options: {'dangerous', 'grievous', 'severe', 'serious'}\n",
      "3'th word's (leg) options: {'BOOSTER_FLAG'}\n",
      "8'th word's (its) options: {'BOOSTER_FLAG'}\n",
      "Number of negations removed : 0\n",
      "--- More options (total: 720) possible ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<text style=color:black>Actual Sentence:  remember my bum leg strikes back this time its serious  :-0.4215</text>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<text style=color:red>Worst Sentence: remember my bum <text style=color:blue><i>[leg]</i></text> <text style=color:blue><i>[absolutely]</i></text> kills back this <text style=color:blue><i>[time]</i></text> <text style=color:blue><i>[its]</i></text> absolutely dangerous : -0.8012</text>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<text style=color:green>Best Sentence: remember my bum <text style=color:blue><i>[leg]</i></text> <text style=color:blue><i>[strikes]</i></text> back this time <text style=color:blue><i>[its]</i></text> <text style=color:blue><i>[definitely]</i></text> serious : -0.0258</text>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num of tweets done : 5\n"
     ]
    }
   ],
   "source": [
    "def extractTwitterDataset():\n",
    "    df_tweets = pd.read_csv( pathToDatasets + 'cleanedTweets.csv', nrows=NUM_OF_TWEETS, skiprows =TWEET_START)\n",
    "    tweets = df_tweets.values\n",
    "    return tweets;\n",
    "\n",
    "def createDictionaryOfSentStrings(sentencesWithSentiment):\n",
    "    dict_sentimentToStringObjects = {}\n",
    "    for obj in sentencesWithSentiment:\n",
    "        sent = obj.getSentiment()\n",
    "        if(dict_sentimentToStringObjects.get(sent) == None):\n",
    "            dict_sentimentToStringObjects[sent] = [obj]\n",
    "        else:\n",
    "            (dict_sentimentToStringObjects[sent]).append(obj)\n",
    "\n",
    "    return dict_sentimentToStringObjects;\n",
    "\n",
    "def runThroughTweets():\n",
    "    tweets = extractTwitterDataset()\n",
    "    \n",
    "    counterOfTweets = 0;\n",
    "    for counter,tweet in enumerate(tweets):\n",
    "        counterOfTweets +=1;\n",
    "        tweet = tweet[0]\n",
    "        tweetTokens = cleanAndTokenizeText(tweet)\n",
    "        mainSentiment = senty.polarity_scores(tweet)['compound']\n",
    "        if(mainSentiment == 0):\n",
    "            continue\n",
    "        print(\"\\n\\n Tweet: {0}:{1}\\n\".format(tweet,mainSentiment))   \n",
    "        \n",
    "        sentenceObj = Sentence(tweet, mainSentiment)\n",
    "        sentenceObj = getAlternativeWords(sentenceObj)\n",
    "        sentenceObj = editBoosterWords(sentenceObj)\n",
    "        \n",
    "        replacementDictionary = sentenceObj.getDictOfIndexWords();\n",
    "        if(len(replacementDictionary) <= 0):        \n",
    "            print(\" -- No new Strings generated ---\\n\\n\")\n",
    "            continue\n",
    "\n",
    "        keysToChange = replacementDictionary.keys();\n",
    "        if(VERBOSE_PRINTING):\n",
    "            for key in keysToChange:\n",
    "                print(\"{0}'th word's ({2}) options: {1}\".format(key,replacementDictionary[key],tweetTokens[key]))\n",
    "\n",
    "        sentenceObj = returnCombinationsOfStrings(sentenceObj)\n",
    "        sentencesWithSentiment, sentenceObj = printStrings(sentenceObj)\n",
    "       \n",
    "        dict_sentimentToStringObjects = createDictionaryOfSentStrings(sentencesWithSentiment)\n",
    "        \n",
    "        sentiments = sorted(dict_sentimentToStringObjects.keys())\n",
    "        \n",
    "    return counterOfTweets;\n",
    "        \n",
    "numOfTweets = runThroughTweets()\n",
    "print(\"num of tweets done : {0}\".format(numOfTweets))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "specificWord = \"good\"\n",
    "def testOneWord(word=\"\"):\n",
    "    if(word==\"\"):\n",
    "        return\n",
    "    print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from IPython.core.debugger import set_trace\n",
    "\n",
    "\n",
    "specificString = \"\"\n",
    "def specificString(textString=\"\"):\n",
    "    if(textString == \"\" or textString == None):\n",
    "        return\n",
    "    textString = dataClean(textString)\n",
    "    textTokens = cleanAndTokenizeText(textString)\n",
    "    mainSentiment = senty.polarity_scores(textString)['compound']\n",
    "    if(mainSentiment == 0):\n",
    "        print(\"{} \\n No sentiment found in sentence\".format(textString));\n",
    "        return;\n",
    "    print(\"\\n {0}:{1}\\n\".format(textString,mainSentiment))   \n",
    "    sentenceObj = Sentence(textString, mainSentiment)\n",
    "    sentenceObj = getAlternativeWords(sentenceObj)\n",
    "    sentenceObj = editBoosterWords(sentenceObj)\n",
    "    \n",
    "    \n",
    "    replacementDictionary = sentenceObj.getDictOfIndexWords();\n",
    "    if(len(replacementDictionary) <= 0):        \n",
    "        print(\" -- No new Strings generated ---\\n\\n\")\n",
    "        return\n",
    "    \n",
    "    keysToChange = replacementDictionary.keys();\n",
    "    if(VERBOSE_PRINTING):\n",
    "        for key in keysToChange:\n",
    "               print(\"{0}'th word's ({2}) options: {1}\".format(key,replacementDictionary[key],textTokens[key]))\n",
    "    \n",
    "    sentenceObj = returnCombinationsOfStrings(sentenceObj)\n",
    "    allPossibleSentences, sentenceObj = printStrings(sentenceObj)\n",
    "    \n",
    "print(\"Enter a string to use\")\n",
    "\n",
    "\n",
    "inputText = \"\"\n",
    "inputText = input()\n",
    "if(inputText == \"\"):\n",
    "    return\n",
    "elif(inputText == \"t\"):\n",
    "#     specificString(\"hot chocolate really angers me and it should sadden you too\")\n",
    "#     specificString(\"the grind is inspirational and saddening at the same time. do not want you to stop cuz i like what u do!\")\n",
    "    specificString(\"Good and bad\");\n",
    "else:\n",
    "    specificString(inputText)\n",
    "# I really love hot chocolate, but I'm not good with hot coffee "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
