{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:85% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gensim.downloader as api\n",
    "import numpy as np\n",
    "import re\n",
    "import csv\n",
    "import pandas as pd\n",
    "import pprint\n",
    "import string\n",
    "import nltk\n",
    "import sys\n",
    "import spacy\n",
    "import os\n",
    "import pickle\n",
    "    \n",
    "from IPython.display import HTML\n",
    "from nltk.corpus import wordnet \n",
    "from sklearn.manifold import TSNE\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "pathToDatasets = '../datasets/'\n",
    "pathToDataScripts = '../datasets/scripts/'\n",
    "filePath = '../datasets/GoogleNews-vectors-negative300.bin'\n",
    "# modelBeingUsed = \"glove-wiki-gigaword-300\"\n",
    "# modelBeingUsed = \"glove-wiki-gigaword-100\"\n",
    "# modelBeingUsed = \"glove-twitter-50\"\n",
    "# modelBeingUsed = \"glove-twitter-100\"\n",
    "modelBeingUsed = \"glove-twitter-200\"\n",
    "\n",
    "sys.path.insert(0, pathToDataScripts)\n",
    "from cleanDataset import tokenize_words, dataClean\n",
    "\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:85% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading binaries and models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Should I reload the model?\n",
      "no\n",
      " didnt reload model! \n",
      "{'should', 'slightly', 'really'}\n"
     ]
    }
   ],
   "source": [
    "print(\"Should I reload the model?\")\n",
    "tstString = input()\n",
    "if(\"no\" in tstString.lower() or \"n\" in tstString.lower()):\n",
    "    print(\" didnt reload model! \")\n",
    "else:\n",
    "    print(\"loading {0}!\".format(modelBeingUsed));\n",
    "    fileName = \"{}.pickle\".format(modelBeingUsed)\n",
    "    if(os.path.exists(pathToDatasets+fileName)):\n",
    "        print(\"loading via pickle!\")\n",
    "        pickle_in = open(pathToDatasets+fileName, \"rb\")\n",
    "        word_vectors = pickle.load(pickle_in);\n",
    "    else:\n",
    "        print(\"Pickle didn't exist, therefore loading model!\")\n",
    "        word_vectors = api.load(\"{0}\".format(modelBeingUsed))\n",
    "        print(\"-- Saving to pickle file for next time! --\")\n",
    "        pickle_out = open(pathToDatasets+fileName,\"wb\")\n",
    "        pickle.dump(word_vectors, pickle_out)\n",
    "        pickle_out.close()\n",
    "    nltk.download('vader_lexicon')\n",
    "    nltk.download('punkt')\n",
    "    nltk.download('averaged_perceptron_tagger')\n",
    "    nltk.download('wordnet')\n",
    "    print(\"Model Loaded!\")\n",
    "\n",
    "setOfBoosterWords = set()\n",
    "with open(pathToDatasets + \"BoosterWordList.txt\") as bwf:\n",
    "    pd_booster = pd.read_csv(bwf, sep='\\t');\n",
    "    listOfBoosterWords = (pd_booster.iloc[:,0]).tolist()\n",
    "    setOfBoosterWords = set(listOfBoosterWords[7:10])\n",
    "    print(setOfBoosterWords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global Variables and Global Objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [],
   "source": [
    "senty = SentimentIntensityAnalyzer()\n",
    "vocabulary = word_vectors.vocab;\n",
    "\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "\n",
    "'''\n",
    "Good sets -> {100+20}\n",
    "'''\n",
    "NUMBER_OF_ALTERNATIVES = 7 \n",
    "TWEET_START = 160\n",
    "NUM_OF_TWEETS = 10\n",
    "\n",
    "\n",
    "# VERBOSE_PRINTING = True\n",
    "VERBOSE_PRINTING = False\n",
    "\n",
    "# USE_SPACY = False\n",
    "USE_SPACY = True\n",
    "\n",
    "COLOR_PRINTING = True\n",
    "# COLOR_PRINTING = False\n",
    "\n",
    "PRINT_NEUTRAL = True\n",
    "# PRINT_NEUTRAL = False\n",
    "\n",
    "# PRINT_ALL_STRINGS = True\n",
    "PRINT_ALL_STRINGS = False\n",
    "\n",
    "SHOW_ALTS = 150\n",
    "\n",
    "punctuation = r\"\\\"#$%&'+-/;<=>?@[\\]*^_`{|}~\"\n",
    "LIST_OF_NEGATIONS = [\"not\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class for Sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [],
   "source": [
    "from SentenceClass import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printStrings(sentenceObj):\n",
    "    \n",
    "    numberOfPrints = 0\n",
    "    newStrings = generateHTMLObjectsFromSentence(sentenceObj)\n",
    "    mainSentiment = sentenceObj.ogSentiment;\n",
    "    listOfSentencesWithSentiments = []\n",
    "    bestSentiment = -sys.maxsize - 1\n",
    "    worstSentiment = sys.maxsize\n",
    "    \n",
    "    bestSentimentString = \"\";\n",
    "    worstSentimentString = \"\";\n",
    "    \n",
    "    for ind, tSentence in enumerate(newStrings):\n",
    "            alteredTweet = tSentence.getSentence()\n",
    "            htmlText = tSentence.getHTML()\n",
    "            \n",
    "            if \"not\" in alteredTweet: continue\n",
    "            \n",
    "            sentimentOfNewString = senty.polarity_scores(alteredTweet)['compound']\n",
    "            newObj = SentenceWithSentiment(alteredTweet, sentimentOfNewString, htmlText)\n",
    "            listOfSentencesWithSentiments.append(newObj)\n",
    "            \n",
    "            if(sentimentOfNewString >= bestSentiment):\n",
    "                bestSentiment = sentimentOfNewString\n",
    "                bestSentimentString = htmlText;\n",
    "            elif(sentimentOfNewString < worstSentiment):\n",
    "                worstSentiment = sentimentOfNewString\n",
    "                worstSentimentString = htmlText;\n",
    "            \n",
    "            \n",
    "            numberOfPrints+=1;\n",
    "            if(numberOfPrints > SHOW_ALTS): break\n",
    "            if(numberOfPrints > SHOW_ALTS or PRINT_ALL_STRINGS == False): continue\n",
    "            if(sentimentOfNewString == mainSentiment or sentimentOfNewString == 0.0):\n",
    "                if(PRINT_ALL_STRINGS and numberOfPrints <= SHOW_ALTS): displayText(\"{0}: {1}\".format(htmlText,sentimentOfNewString),'black')\n",
    "            elif(sentimentOfNewString > mainSentiment):\n",
    "                if(PRINT_ALL_STRINGS and numberOfPrints <= SHOW_ALTS): displayText(\"{0}: {1}\".format(htmlText,sentimentOfNewString),'green')\n",
    "            elif(sentimentOfNewString < mainSentiment and sentimentOfNewString != 0.0):\n",
    "                if(PRINT_ALL_STRINGS and numberOfPrints <= SHOW_ALTS): displayText(\"{0}: {1}\".format(htmlText,sentimentOfNewString),'red')\n",
    "                \n",
    "    if(numberOfPrints > SHOW_ALTS): print(\"--- More options (total: {0}) possible, but not printed ---\".format(numberOfPrints));\n",
    "    displayText(\"Actual Sentence: {0} :{1}\".format(sentenceObj.ogSentence, sentenceObj.ogSentiment))\n",
    "    if (worstSentimentString != \"\"): displayText(\"Worst Sentence: {0} : {1}\".format(worstSentimentString, worstSentiment), color='red')\n",
    "    if (bestSentimentString != \"\"): displayText(\"Best Sentence: {0} : {1}\".format(bestSentimentString, bestSentiment), color='green') \n",
    "    \n",
    "    \n",
    "    sentenceObj.addFinalSentences(listOfSentencesWithSentiments)\n",
    "    return listOfSentencesWithSentiments, sentenceObj;\n",
    "\n",
    "def cstr(s, color='black', italics=False):\n",
    "    if(COLOR_PRINTING):\n",
    "        if(italics):\n",
    "            return cstr(\"<i>{0}</i>\".format(s), color);\n",
    "        return \"<text style=color:{}>{}</text>\".format(color, s)\n",
    "    else:\n",
    "        return \"{}\".format(s)\n",
    "\n",
    "def displayText(text, color='black'):\n",
    "    if(COLOR_PRINTING):\n",
    "        display(HTML(cstr(text, color)));\n",
    "        return\n",
    "    print(\"{}\".format(text));\n",
    "    \n",
    "    \n",
    "def cleanAndTokenizeText(text):\n",
    "    text = text.lower();\n",
    "    newString = \"\"\n",
    "    for char in text:\n",
    "        if char not in punctuation:\n",
    "            newString += char\n",
    "    text = word_tokenize(newString)\n",
    "    return text;\n",
    "\n",
    "def getPOSTags(tweet):\n",
    "    if(USE_SPACY == False):\n",
    "        tags = nltk.pos_tag(tweet)\n",
    "        return tags;    \n",
    "    tweet = ' '.join(tweet)\n",
    "    doc = nlp(tweet)\n",
    "    tags = [(token.text, token.tag_) for token in doc] # since the format expected is [text,tag]\n",
    "    return tags\n",
    "\n",
    "def getAntonymsAndSynonymsOfWords(word):\n",
    "    if(word not in vocabulary):\n",
    "        return [], []\n",
    "    setOfAntonyms = set()\n",
    "    setOfSynonyms = set()\n",
    "    for syn in wordnet.synsets(word):\n",
    "        for l in syn.lemmas():\n",
    "            setOfSynonyms.add(l.name().lower())\n",
    "            anton = l.antonyms()\n",
    "            if(anton!=[]):\n",
    "                setOfAntonyms.add(anton[0].name().lower())\n",
    "    if(len(setOfAntonyms) == 0):\n",
    "        if(VERBOSE_PRINTING): print(\"No antonyms found for word {0}\".format(word))\n",
    "    if(len(setOfSynonyms) == 0):\n",
    "        if(VERBOSE_PRINTING): print(\"No synonyms found for word {0}\".format(word))\n",
    "            \n",
    "    return list(setOfAntonyms), list(setOfSynonyms)\n",
    "\n",
    "\n",
    "def returnReplacementsForWord(word):\n",
    "    \n",
    "    if(word not in vocabulary):\n",
    "        print(\" --- {0} not in vocabulary ---\".format(word))\n",
    "        return []\n",
    "    possibleReplacements = [word[0] for word in word_vectors.most_similar(word,topn=NUMBER_OF_ALTERNATIVES)]\n",
    "    \n",
    "    if(possibleReplacements == []):\n",
    "        print(\" --- No replacements for word {0} ---\".format(word))\n",
    "    antonyms,synonyms = getAntonymsAndSynonymsOfWords(word)\n",
    "    \n",
    "    if(antonyms != []):\n",
    "        possibleReplacements.extend(antonyms)\n",
    "        if(VERBOSE_PRINTING): print(\"Some antonyms for word {0} are {1}\".format(word, antonyms[:3]))\n",
    "    if(synonyms != []):\n",
    "        possibleReplacements.extend(synonyms)\n",
    "        if(VERBOSE_PRINTING): print(\"Some synonyms for word {0} are {1}\".format(word, synonyms[:3]))\n",
    "    return possibleReplacements\n",
    "    \n",
    "def posApprovedReplacements(alternativeWords, userTokens, indexOfToken):\n",
    "    if(alternativeWords == []):\n",
    "        return []\n",
    "    tempTokens = userTokens[:]\n",
    "    POSTokens = getPOSTags(tempTokens)\n",
    "    validWords = []\n",
    "    \n",
    "    mainTag = POSTokens[indexOfToken][1]\n",
    "    mainWord = userTokens[indexOfToken]\n",
    "    \n",
    "    for ind,word in enumerate(alternativeWords):\n",
    "        if(\"_\" in word): continue\n",
    "        tempTokens[indexOfToken] = word\n",
    "        posTags = getPOSTags(tempTokens)\n",
    "        newTag = (posTags[indexOfToken])[1]\n",
    "        if(str(newTag) == str(mainTag)):\n",
    "#             if(VERBOSE_PRINTING): print(\"Word {0}[{1}] matched with {2}[{3}]\".format(mainWord, mainTag, word,newTag))\n",
    "            validWords.append(word)\n",
    "    if(validWords == [] and VERBOSE_PRINTING):\n",
    "        print(\"No POS words found for word {} with tag {}\".format(mainWord, mainTag));\n",
    "    return validWords\n",
    "    \n",
    "def editBoosterWords(sentenceObj):\n",
    "    ogSentence = sentenceObj.ogSentence;\n",
    "    \n",
    "    tokenizedSentence = cleanAndTokenizeText(ogSentence)\n",
    "    \n",
    "    '''\n",
    "        Use the indexToAlternatives dictionary to add words into the sentences, as part of the chunking process \n",
    "        The problem with this instinctively is how do you have an index \"between\" two variables \n",
    "        You can't really. You'd have to have a flag that says which places it's meant to go \n",
    "        You could do it _before_ the sentiment heavy word (this makes more natural sense) or \n",
    "            you could do it _after_ the word before the sentiment heavy word (this could have its own advantages) \n",
    "    '''\n",
    "    \n",
    "    posTagsForWords = getPOSTags(tokenizedSentence)    \n",
    "    for ind,(word,tag) in enumerate(posTagsForWords):    \n",
    "        if(word in setOfBoosterWords):\n",
    "            tmp = ' '.join(tokenizedSentence[ind:ind+3])\n",
    "            if(senty.polarity_scores(tmp)['compound'] != 0):\n",
    "                sentenceObj.addAlternativesByIndex(ind, [\"IGNORE_FLAG\", word]);\n",
    "                continue\n",
    "        if(senty.polarity_scores(word)['compound'] != 0):\n",
    "            print(\"Possible Booster Word and Tag: {0}-{1}\".format(word,tag))\n",
    "        if(senty.polarity_scores(word)['compound'] != 0 and (\"RB\" in tag or \"JJ\" in tag)):  # \"JJ\" in tag and\n",
    "            newInd = ind-1 if ((ind-1) >=0 ) else 0\n",
    "            sentenceObj.addAlternativesByIndex(newInd, [\"BOOSTER_FLAG\"]);\n",
    "            print(\"Placed booster at {0} from word {1} on word {2}\".format(newInd, word,tokenizedSentence[newInd] ))\n",
    "    \n",
    "    return sentenceObj\n",
    "def generateHTMLObjectsFromSentence(sentenceObj):\n",
    "    \n",
    "    allSentences = sentenceObj.getFinalSentences()\n",
    "    indexToAlts = sentenceObj.indexToSetOfWords;\n",
    "    indexToChange = list(indexToAlts.keys());\n",
    "    \n",
    "    numberOfNegationsRemoved = 0;\n",
    "    listOfSentenceObjs = []\n",
    "    for t_sentenceObj in allSentences:\n",
    "        sentence = t_sentenceObj.getSentence()\n",
    "        sentence = t_sentenceObj.getSentence()\n",
    "        copySentence = cleanAndTokenizeText(sentence)\n",
    "        subIndex = 0\n",
    "        for index in sorted(indexToChange):\n",
    "            if(index > len(copySentence)):  \n",
    "                copySentence[len(copySentence) - index] = cstr(\"[{0}]\".format(copySentence[len(copySentence) - index]), \"blue\", italics=True);\n",
    "                continue\n",
    "            else:\n",
    "                if(\"IGNORE_FLAG\" in indexToAlts[index]):\n",
    "                    if(copySentence[index] in LIST_OF_NEGATIONS):\n",
    "                        continue; ## Word that _should_ be removed is present.\n",
    "                    else:\n",
    "                        subIndex +=1\n",
    "                        numberOfNegationsRemoved +=1;\n",
    "                        continue;\n",
    "                if(\"BOOSTER_FLAG\" in indexToAlts[index]):\n",
    "                    newInd = index if index < len(copySentence) else len(copySentence)-1 \n",
    "                    copySentence[newInd] = cstr(\"[{0}]\".format(copySentence[newInd]), \"blue\", italics=True);\n",
    "                    continue\n",
    "            copySentence[index-subIndex] = cstr(\"[{0}]\".format(copySentence[index-subIndex]), \"blue\", italics=True);\n",
    "        listOfSentenceObjs.append(SentenceWithHTML(sentence, ' '.join(copySentence)));\n",
    "    \n",
    "    if(VERBOSE_PRINTING): print(\"Number of negations removed : {0}\".format(numberOfNegationsRemoved))\n",
    "    return listOfSentenceObjs\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence Chunking and Appending"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [],
   "source": [
    "def helper_combine(mainList, myList):\n",
    "    '''\n",
    "    helper function for CombineSentenceChunks\n",
    "    '''\n",
    "    newList = []\n",
    "    for val in myList:\n",
    "        for mainVal in mainList:\n",
    "#             if(VERBOSE_PRINTING): print(\"Combining {0} with {1}\".format(' '.join(val), ' '.join(mainVal)));\n",
    "            newList.append(val + mainVal);\n",
    "    return newList;\n",
    "\n",
    "def combineSentenceChunks(wholeSentence, dictOfChunks):\n",
    "    '''\n",
    "        Uses the helper_combine function to generate all possible combinatios and permuations of the chunks\n",
    "        and any alternatives.\n",
    "        \n",
    "        The logic is to use the end of the sentence, apply each possible chunk from the previous key's chunks\n",
    "        to every possible chunk of this key's.\n",
    "        \n",
    "        The helper function is used to allow us to reuse the list of alreadyGeneratedChunks and constantly\n",
    "        append to them.\n",
    "        \n",
    "        To understand the logic better, take a look at this gist:\n",
    "        https://gist.github.com/sunnyMiglani/cf85407a9e6928237b1436cc2bc95fa4\n",
    "        \n",
    "    '''\n",
    "    reversedKeys = sorted(dictOfChunks.keys(), reverse=True)\n",
    "    completeSentences = [];\n",
    "    mainArr = dictOfChunks[reversedKeys[0]]\n",
    "    for ind in range(1, len(reversedKeys)):\n",
    "        key = reversedKeys[ind];\n",
    "        mainArr = helper_combine(mainArr, dictOfChunks[key]);\n",
    "        \n",
    "    return mainArr;\n",
    "        \n",
    "def generateSentenceChunks(wholeSentence, keyToChange, nextKey, listOfMyAlternatives):\n",
    "    '''\n",
    "        Generates sentence chunks by iterating through the list of alternatives\n",
    "        Chunking the sentence to start from current key to next key.\n",
    "        This means that the sentence always goes from key 'x' to key 'y'\n",
    "        \n",
    "        Example:\n",
    "        \"I really <hate> hot chocolate, but I <prefer> hot coffee\"\n",
    "        Calling generateSentenceChunks will create an example sentence:\n",
    "            - \"<altWordForHate> hot chocolate , but I \"\n",
    "        \n",
    "        Remember to append the first stretch of the string to the first key's chunk for proper use!\n",
    "    '''\n",
    "    \n",
    "    newList = list(listOfMyAlternatives)\n",
    "    newList.append(wholeSentence[keyToChange]);\n",
    "    generatedSentences = []\n",
    "    \n",
    "        \n",
    "    for myAlt in newList:\n",
    "        if(\"IGNORE_FLAG\" == myAlt):\n",
    "            newSentence = wholeSentence[keyToChange+1:nextKey];\n",
    "            generatedSentences.append(newSentence)\n",
    "            continue\n",
    "        elif(\"BOOSTER_FLAG\" == myAlt): \n",
    "            print(\"booster flag found at word:{0}, ind:{1}\".format(wholeSentence[keyToChange], keyToChange))\n",
    "            for booster in setOfBoosterWords:\n",
    "                newSentence = wholeSentence[:]\n",
    "                newSentence.insert(keyToChange+1, booster)\n",
    "#                 print(\"New sentence generated : {0}\".format(' '.join(newSentence)))\n",
    "                generatedSentences.append(newSentence[keyToChange:nextKey+1]);\n",
    "#                 print(\"Sentence added :{0}\".format(newSentence[keyToChange:nextKey+1]))\n",
    "        else:\n",
    "            newSentence = wholeSentence[:]\n",
    "            newSentence[keyToChange] = myAlt\n",
    "            generatedSentences.append(newSentence[keyToChange:nextKey]);\n",
    "        \n",
    "    return generatedSentences\n",
    "    \n",
    "def returnCombinationsOfStrings(sentenceObj):\n",
    "    \n",
    "    indexToWordDict = sentenceObj.indexToSetOfWords;\n",
    "    originalSentence = sentenceObj.ogSentence;\n",
    "    tokenizedSentence = cleanAndTokenizeText(originalSentence)\n",
    "    reversedKeys = sorted(indexToWordDict.keys(), reverse=True)\n",
    "    dictAlternatives  = {}\n",
    "\n",
    "    sortedKeys = sorted(indexToWordDict.keys())\n",
    "    sentenceChunks = {}\n",
    "    htmlChunks = {}\n",
    "    \n",
    "    for ind in range(0,len(sortedKeys)):\n",
    "        key = sortedKeys[ind]\n",
    "        nextKey = sortedKeys[ind+1] if ind+1 < len(sortedKeys) else len(tokenizedSentence)\n",
    "        sentenceChunks[key] = generateSentenceChunks(tokenizedSentence, key, nextKey, indexToWordDict[key])\n",
    "\n",
    "    if(sortedKeys[0] != 0):\n",
    "        newList = []\n",
    "        for thislist in sentenceChunks[sortedKeys[0]]:\n",
    "            newList.append(tokenizedSentence[0:sortedKeys[0]] + thislist)\n",
    "        sentenceChunks[sortedKeys[0]] = newList;\n",
    "        \n",
    "    finalOptions = combineSentenceChunks(tokenizedSentence, sentenceChunks)\n",
    "    \n",
    "    finalSentences = []\n",
    "    for val in finalOptions:\n",
    "        sentence = ' '.join(val)\n",
    "        sentiment = senty.polarity_scores(sentence)['compound']\n",
    "        finalSentences.append(SentenceWithSentiment(sentence,sentiment))\n",
    "    \n",
    "    sentenceObj.addFinalSentences(finalSentences)   \n",
    "    return sentenceObj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAlternativeWords(sentenceObj):\n",
    "    mainSentence = sentenceObj.ogSentence;\n",
    "    mainSentiment = sentenceObj.ogSentiment;\n",
    "\n",
    "    sentenceTokens = cleanAndTokenizeText(mainSentence)\n",
    "\n",
    "    for ind, word in enumerate(sentenceTokens):\n",
    "        alternativeSentenceWithHTML = []\n",
    "        copyOfTokens = sentenceTokens[:]\n",
    "        replacements = []\n",
    "        replacements = sentenceObj.checkIfWordExists(word)\n",
    "        if(replacements != []):\n",
    "            print(\"FOUND REPEATED WORD {0}\".format(word))\n",
    "            sentenceObj.addAlternativesByIndex(ind, replacements)\n",
    "            continue;\n",
    "        \n",
    "        if(word in LIST_OF_NEGATIONS):\n",
    "            '''\n",
    "                If the word is in the list of negations\n",
    "                We add a [IGNORE_FLAG] tag to the alternatives\n",
    "                Which will be grabbed in the future by the sentence generation algorithm. (chunking)\n",
    "            '''\n",
    "            tmp = ' '.join(sentenceTokens[ind:ind+3])\n",
    "            if(senty.polarity_scores(tmp)['compound'] != 0):\n",
    "                sentenceObj.addAlternativesByIndex(ind, [\"IGNORE_FLAG\", word]);\n",
    "                continue\n",
    "        \n",
    "        score = senty.polarity_scores(word)['compound']\n",
    "        if(score != 0.0):\n",
    "            tempReplacements = returnReplacementsForWord(word) # get embedding based relations\n",
    "            if(VERBOSE_PRINTING): print(\"Some early replacements for word [{0}] are {1}\".format(word, tempReplacements[0:8]))\n",
    "            if(tempReplacements == []):\n",
    "                print(\"No replacements found at all for word {0}\".format(word))\n",
    "                continue\n",
    "            replacements = posApprovedReplacements(tempReplacements[:], copyOfTokens[:], ind)\n",
    "            finalReplacements = []\n",
    "            if(VERBOSE_PRINTING): skippedWords = []\n",
    "            for new_word in replacements:\n",
    "                thisSentiment = senty.polarity_scores(new_word)['compound']\n",
    "                if(thisSentiment != 0.0):\n",
    "                    finalReplacements.append(new_word)\n",
    "                else:\n",
    "                    if(VERBOSE_PRINTING):\n",
    "                        skippedWords.append(new_word)\n",
    "            if(VERBOSE_PRINTING and len(skippedWords) > 0):\n",
    "                print(\"some skipped words: {0}\".format(skippedWords[:5]));\n",
    "            if(finalReplacements == []):\n",
    "                if(VERBOSE_PRINTING): print(\" -- No POS approved words! -- for word {0}\\n some non-POS:{1}\".format(word, tempReplacements[:4]))\n",
    "                continue\n",
    "            sentenceObj.addAlternativesByIndex(ind, finalReplacements)\n",
    "            sentenceObj.addWordToAlternatives(word, finalReplacements)\n",
    "    return sentenceObj\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interactionHelper(dictOfAltObjs, sentiments, thisSent, lastWorkingIndex=0):\n",
    "    if(thisSent in sentiments):\n",
    "        mainIndex = sentiments.index(thisSent)\n",
    "        \n",
    "    else:\n",
    "        mainIndex = lastWorkingIndex\n",
    "    \n",
    "    numLower =  mainIndex  # Mod to keep it in range 0 -> val \n",
    "    numHigher = len(sentiments) - mainIndex - 1\n",
    "\n",
    "    print(\"There are totally {0} possible sentences\".format(len(sentiments)))\n",
    "    print(\"Current sentiment is at index {0}, and length of sentiments is {1}\".format(mainIndex, len(sentiments)));\n",
    "    print(\" <--- {0} lower and {1} ---> higher \".format(numLower, numHigher))\n",
    "    print(\"Would you like to see a lower sentiment or higher sentiment or ALL?\")\n",
    "    print(\"Please enter 1 for lower, 2 for higher or 0 for all\")\n",
    "    \n",
    "    targetSentiment = thisSent\n",
    "    textInput = input()\n",
    "    if(textInput == \"\"):\n",
    "        print(\"You entered nothing! exiting\")\n",
    "        return\n",
    "        \n",
    "    if(textInput.isdigit()):\n",
    "        textInput = int(textInput)\n",
    "        if(textInput == 0):\n",
    "            if(VERBOSE_PRINTING): print(\"Printing sentiments from : {0}\".format(sentiments))\n",
    "            for sentiment in sentiments:\n",
    "                obj = dictOfAltObjs[sentiment][0];\n",
    "                displayText(\"{0}  : {1}\".format(obj.getHTML(), sentiment))\n",
    "            return\n",
    "        elif(textInput == 1 and numLower > 0 and (mainIndex - 1) >= 0):\n",
    "            targetSentiment = sentiments[mainIndex - 1]\n",
    "            htmlSentence = dictOfAltObjs[targetSentiment][0].getHTML();\n",
    "            displayText(\"{0} : {1}\".format(htmlSentence, targetSentiment))\n",
    "            \n",
    "        elif(textInput == 2 and numHigher < len(sentiments) and (mainIndex + 1) < len(sentiments)):\n",
    "            targetSentiment = sentiments[mainIndex + 1]\n",
    "            htmlSentence = dictOfAltObjs[targetSentiment][0].getHTML();\n",
    "            displayText(\"{0} : {1}\".format(htmlSentence, targetSentiment))\n",
    "            \n",
    "        else:\n",
    "            print(\"You entered an invalid option\")\n",
    "            interactionHelper(dictOfAltObjs, sentiments, thisSent, mainIndex)\n",
    "            return\n",
    "        \n",
    "    interactionHelper(dictOfAltObjs, sentiments, targetSentiment,mainIndex)\n",
    "\n",
    "\n",
    "\n",
    "def interactionWithUser(mainSentenceObj, dictOfAltObjs,sentiments, isUser=False):\n",
    "    if(isUser == False):\n",
    "        return\n",
    "    else:\n",
    "        mainSentence = mainSentenceObj.ogSentence;\n",
    "        mainSent = mainSentenceObj.ogSentiment;\n",
    "        if(VERBOSE_PRINTING):\n",
    "            print(\"The following are the sentiment values for the input\")\n",
    "            for ind,sent in enumerate(sentiments):\n",
    "                print(\"{0} - {1}\".format(ind+1,sent))\n",
    "        \n",
    "        interactionHelper(dictOfAltObjs, sentiments, mainSent) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " Tweet: picked mich st to win it all from the get go.  was feeling pretty good about that pick all the way up until......tonight.  a is lost too :0.8442\n",
      "\n",
      "Possible Booster Word and Tag: win-VB\n",
      "Possible Booster Word and Tag: feeling-VBG\n",
      "Possible Booster Word and Tag: pretty-RB\n",
      "Placed booster at 13 from word pretty on word feeling\n",
      "Possible Booster Word and Tag: good-JJ\n",
      "Placed booster at 14 from word good on word pretty\n",
      "Possible Booster Word and Tag: lost-VBN\n",
      "booster flag found at word:feeling, ind:13\n",
      "booster flag found at word:pretty, ind:14\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-426-1c548e4afcef>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mcounterOfTweets\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m \u001b[0mnumOfTweets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrunThroughTweets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"num of tweets done : {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumOfTweets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-426-1c548e4afcef>\u001b[0m in \u001b[0;36mrunThroughTweets\u001b[0;34m()\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0msentenceObj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreturnCombinationsOfStrings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentenceObj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m         \u001b[0msentencesWithSentiment\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentenceObj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprintStrings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentenceObj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0mdict_sentimentToStringObjects\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreateDictionaryOfSentStrings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentencesWithSentiment\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-422-d41309b88b28>\u001b[0m in \u001b[0;36mprintStrings\u001b[0;34m(sentenceObj)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mnumberOfPrints\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mnewStrings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerateHTMLObjectsFromSentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentenceObj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mmainSentiment\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msentenceObj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mogSentiment\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mlistOfSentencesWithSentiments\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-422-d41309b88b28>\u001b[0m in \u001b[0;36mgenerateHTMLObjectsFromSentence\u001b[0;34m(sentenceObj)\u001b[0m\n\u001b[1;32m    180\u001b[0m         \u001b[0msentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt_sentenceObj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetSentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m         \u001b[0msentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt_sentenceObj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetSentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m         \u001b[0mcopySentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcleanAndTokenizeText\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m         \u001b[0msubIndex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexToChange\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-422-d41309b88b28>\u001b[0m in \u001b[0;36mcleanAndTokenizeText\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mchar\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpunctuation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m             \u001b[0mnewString\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mchar\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m     \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnewString\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    141\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mtype\u001b[0m \u001b[0mpreserve_line\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m     \"\"\"\n\u001b[0;32m--> 143\u001b[0;31m     \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpreserve_line\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    144\u001b[0m     return [\n\u001b[1;32m    145\u001b[0m         \u001b[0mtoken\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentences\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_treebank_word_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m    103\u001b[0m     \"\"\"\n\u001b[1;32m    104\u001b[0m     \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'tokenizers/punkt/{0}.pickle'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[1;32m   1267\u001b[0m         \u001b[0mGiven\u001b[0m \u001b[0ma\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturns\u001b[0m \u001b[0ma\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0msentences\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthat\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1268\u001b[0m         \"\"\"\n\u001b[0;32m-> 1269\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentences_from_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1270\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1271\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdebug_decisions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36msentences_from_text\u001b[0;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0mfollows\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mperiod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1322\u001b[0m         \"\"\"\n\u001b[0;32m-> 1323\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspan_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_slices_from_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0mfollows\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mperiod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1322\u001b[0m         \"\"\"\n\u001b[0;32m-> 1323\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspan_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_slices_from_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36mspan_tokenize\u001b[0;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[1;32m   1311\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1312\u001b[0m             \u001b[0mslices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_realign_boundaries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mslices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1313\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0msl\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mslices\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1314\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1315\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m_realign_boundaries\u001b[0;34m(self, text, slices)\u001b[0m\n\u001b[1;32m   1353\u001b[0m         \u001b[0mrealign\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1354\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0msl1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msl2\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_pair_iter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mslices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1355\u001b[0;31m             \u001b[0msl1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mslice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msl1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mrealign\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msl1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1356\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msl2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1357\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msl1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def extractTwitterDataset():\n",
    "    df_tweets = pd.read_csv( pathToDatasets + 'cleanedTweets.csv', nrows=NUM_OF_TWEETS, skiprows =TWEET_START)\n",
    "    tweets = df_tweets.values\n",
    "    return tweets;\n",
    "\n",
    "def createDictionaryOfSentStrings(sentencesWithSentiment):\n",
    "    dict_sentimentToStringObjects = {}\n",
    "    for obj in sentencesWithSentiment:\n",
    "        sent = obj.getSentiment()\n",
    "        if(dict_sentimentToStringObjects.get(sent) == None):\n",
    "            dict_sentimentToStringObjects[sent] = [obj]\n",
    "        else:\n",
    "            (dict_sentimentToStringObjects[sent]).append(obj)\n",
    "\n",
    "    return dict_sentimentToStringObjects;\n",
    "\n",
    "def runThroughTweets():\n",
    "    tweets = extractTwitterDataset()\n",
    "    \n",
    "    counterOfTweets = 0;\n",
    "    for counter,tweet in enumerate(tweets):\n",
    "        counterOfTweets +=1;\n",
    "        tweet = tweet[0]\n",
    "        tweetTokens = cleanAndTokenizeText(tweet)\n",
    "        mainSentiment = senty.polarity_scores(tweet)['compound']\n",
    "        if(mainSentiment == 0):\n",
    "            continue\n",
    "        print(\"\\n\\n Tweet: {0}:{1}\\n\".format(tweet,mainSentiment))   \n",
    "        sentenceObj = Sentence(tweet, mainSentiment)\n",
    "        sentenceObj = getAlternativeWords(sentenceObj)\n",
    "        sentenceObj = editBoosterWords(sentenceObj)\n",
    "        replacementDictionary = sentenceObj.getDictOfIndexWords();\n",
    "        if(len(replacementDictionary) <= 0):        \n",
    "            print(\" -- No new Strings generated ---\\n\\n\")\n",
    "            continue\n",
    "\n",
    "        keysToChange = replacementDictionary.keys();\n",
    "        if(VERBOSE_PRINTING):\n",
    "            for key in keysToChange:\n",
    "                print(\"{0}'th word's ({2}) options: {1}\".format(key,replacementDictionary[key],tweetTokens[key]))\n",
    "\n",
    "        sentenceObj = returnCombinationsOfStrings(sentenceObj)\n",
    "        sentencesWithSentiment, sentenceObj = printStrings(sentenceObj)\n",
    "       \n",
    "        dict_sentimentToStringObjects = createDictionaryOfSentStrings(sentencesWithSentiment)\n",
    "        \n",
    "        sentiments = sorted(dict_sentimentToStringObjects.keys())\n",
    "        \n",
    "    return counterOfTweets;\n",
    "        \n",
    "numOfTweets = runThroughTweets()\n",
    "print(\"num of tweets done : {0}\".format(numOfTweets))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "specificWord = \"good\"\n",
    "def testOneWord(word=\"\"):\n",
    "    if(word==\"\"):\n",
    "        return\n",
    "    print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "specificString = \"\"\n",
    "def specificString(textString=\"\"):\n",
    "    if(textString == \"\" or textString == None):\n",
    "        return\n",
    "    textString = dataClean(textString)\n",
    "    textTokens = cleanAndTokenizeText(textString)\n",
    "    mainSentiment = senty.polarity_scores(textString)['compound']\n",
    "    if(mainSentiment == 0):\n",
    "        print(\"{} \\n No sentiment found in sentence\".format(textString));\n",
    "        return;\n",
    "    print(\"\\n {0}:{1}\\n\".format(textString,mainSentiment))   \n",
    "    sentenceObj = Sentence(textString, mainSentiment)\n",
    "    sentenceObj = getAlternativeWords(sentenceObj)\n",
    "    sentenceObj = editBoosterWords(sentenceObj)\n",
    "    \n",
    "    \n",
    "    replacementDictionary = sentenceObj.getDictOfIndexWords();\n",
    "    if(len(replacementDictionary) <= 0):        \n",
    "        print(\" -- No new Strings generated ---\\n\\n\")\n",
    "        return\n",
    "    \n",
    "    keysToChange = replacementDictionary.keys();\n",
    "    if(VERBOSE_PRINTING):\n",
    "        for key in keysToChange:\n",
    "               print(\"{0}'th word's ({2}) options: {1}\".format(key,replacementDictionary[key],textTokens[key]))\n",
    "    \n",
    "    sentenceObj = returnCombinationsOfStrings(sentenceObj)\n",
    "    allPossibleSentences, sentenceObj = printStrings(sentenceObj)\n",
    "    \n",
    "    \n",
    "    dict_sentimentToStringObjects = createDictionaryOfSentStrings(allPossibleSentences)\n",
    "        \n",
    "    sentiments = sorted(dict_sentimentToStringObjects.keys())\n",
    "\n",
    "print(\"Enter a string to use\")\n",
    "\n",
    "\n",
    "inputText = \"\"\n",
    "if(inputText == \"\"):\n",
    "    return\n",
    "elif(inputText == \"t\"):\n",
    "    specificString(\"I really love hot chocolate, but I'm not good with hot coffee\")\n",
    "#     specificString(\"damn! the grind is inspirational and saddening at the same time. do not want you to stop cuz i like what u do! much love\");\n",
    "#     specificString(\"the grind is inspirational and saddening at the same time. do not want you to stop cuz i like what u do!\")\n",
    "else:\n",
    "    specificString(inputText)\n",
    "# I really love hot chocolate, but I'm not good with hot coffee "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "this is making me feel bad:-0.5423\n",
    "this is making me feel a bit bad:-0.5423\n",
    "\n",
    "this is making me feel very bad:-0.5849\n",
    "this is making me feel quite bad:-0.5849\n",
    "this is making me feel incredibly bad:-0.5849\n",
    "\n",
    "this is making me feel kinda bad:-0.4951\n",
    "this is making me feel slightly bad:-0.4951\n",
    "this is making me feel barely bad:-0.4951\n",
    "\n",
    "this is making me feel not bad:0.431\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
