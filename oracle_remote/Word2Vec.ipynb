{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from sklearn.manifold import TSNE\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "import gensim.downloader as api\n",
    "import numpy as np\n",
    "import re\n",
    "import csv\n",
    "\n",
    "import pandas as pd\n",
    "import pprint\n",
    "\n",
    "import string\n",
    "import nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pathToDatasets = '../datasets/'\n",
    "filePath = '../datasets/GoogleNews-vectors-negative300.bin'\n",
    "word_vectors = api.load(\"glove-wiki-gigaword-100\")\n",
    "nltk.download('vader_lexicon')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/nltk/twitter/__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n",
      "  warnings.warn(\"The twython library has not been installed. \"\n"
     ]
    }
   ],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "\n",
    "\n",
    "senty = SentimentIntensityAnalyzer()\n",
    "vocabulary = word_vectors.vocab;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def listReplacements(word):\n",
    "    if(word not in vocabulary):\n",
    "        return []\n",
    "    possibleReplacements = [word[0] for word in word_vectors.most_similar(word,topn=5)]\n",
    "    \n",
    "    \n",
    "    return possibleReplacements\n",
    "\n",
    "def cleanAndTokenizeText(text):\n",
    "    text = text.lower()\n",
    "    newString = \"\"\n",
    "    for char in text:\n",
    "        if char not in string.punctuation:\n",
    "            newString += char\n",
    "    text = word_tokenize(newString)\n",
    "    return text;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAlternativeSentences(tweet, sentimentOfTweet):\n",
    "    userInputTokens = cleanAndTokenizeText(tweet)\n",
    "    print(\"Alternatives: \")\n",
    "   \n",
    "    alternativeStrings = []\n",
    "    for ind,word in enumerate(userInputTokens):\n",
    "        \n",
    "        score = senty.polarity_scores(word)['compound']\n",
    "        newUserTokens = userInputTokens[:]\n",
    "        \n",
    "        if(score != 0.0 ):\n",
    "            replacements = listReplacements(word)       \n",
    "            if(replacements == []):\n",
    "                return\n",
    "        \n",
    "            replacements = posApprovedReplacements(replacements[:], newUserTokens[:], ind)\n",
    "            print(\"Word being changed : {0}\".format(word));\n",
    "            for newWord in replacements:\n",
    "                \n",
    "                newUserTokens[ind] = newWord;\n",
    "                newString = ' '.join(newUserTokens)\n",
    "                sentimentOfNewString = senty.polarity_scores(newString)['compound']\n",
    "             \n",
    "            \n",
    "                if(sentimentOfNewString == 0):\n",
    "                    continue\n",
    "                \n",
    "                \n",
    "                \n",
    "                alternativeStrings.append(newString)\n",
    "                if((sentimentOfNewString) >= (sentimentOfTweet)):\n",
    "                    print(\"{0} : {1} ++POS++\".format(newString,sentimentOfNewString))\n",
    "                else:\n",
    "                    print(\"{0} : {1} --NEG--\".format(newString,sentimentOfNewString))\n",
    "            newUserTokens = userInputTokens[:];\n",
    "    return alternativeStrings;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def posApprovedReplacements(alternativeWords, userTokens, indexOfToken):\n",
    "    tempTokens = userTokens[:]\n",
    "    truePOSTokens = getPOSTags(tempTokens)\n",
    "    validWords = []\n",
    "    \n",
    "    mainTag = truePOSTokens[indexOfToken]\n",
    "    print(\"Main tag : {0}\".format(mainTag[1]))\n",
    "#     pprint.pprint(truePOSTokens)\n",
    "    \n",
    "    for ind,word in enumerate(alternativeWords):\n",
    "        print(\"Changing word:  {0} to {1}\\n\".format(userTokens[indexOfToken], word))\n",
    "        tempTokens[indexOfToken] = word\n",
    "        posTags = getPOSTags(tempTokens)\n",
    "        newTag = (posTags[indexOfToken])[1]\n",
    "        print(\"New Tag : {0}\".format(newTag))\n",
    "        if(str(newTag) == str(mainTag)):\n",
    "            validWords.append(word)\n",
    "        pprint.pprint(validWords)\n",
    "    return validWords\n",
    "        \n",
    "\n",
    "\n",
    "def getPOSTags(tweet):\n",
    "    tags = nltk.pos_tag(tweet)\n",
    "    return(tags)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-NUM(0) i really hate how people diss my bands!  trace is clearly not ugly!:0.2484-\n",
      "\n",
      "Alternatives: \n",
      "Main tag : VB\n",
      "Changing word:  hate to hatred\n",
      "\n",
      "New Tag : VBD\n",
      "[]\n",
      "Changing word:  hate to racist\n",
      "\n",
      "New Tag : VB\n",
      "[]\n",
      "Changing word:  hate to racism\n",
      "\n",
      "New Tag : VB\n",
      "[]\n",
      "Changing word:  hate to anyone\n",
      "\n",
      "New Tag : NN\n",
      "[]\n",
      "Changing word:  hate to fear\n",
      "\n",
      "New Tag : VB\n",
      "[]\n",
      "Word being changed : hate\n",
      "Main tag : RB\n",
      "Changing word:  clearly to obviously\n",
      "\n",
      "New Tag : RB\n",
      "[]\n",
      "Changing word:  clearly to certainly\n",
      "\n",
      "New Tag : RB\n",
      "[]\n",
      "Changing word:  clearly to seems\n",
      "\n",
      "New Tag : VBZ\n",
      "[]\n",
      "Changing word:  clearly to indeed\n",
      "\n",
      "New Tag : RB\n",
      "[]\n",
      "Changing word:  clearly to seemed\n",
      "\n",
      "New Tag : VBN\n",
      "[]\n",
      "Word being changed : clearly\n",
      "Main tag : RB\n",
      "Changing word:  ugly to nasty\n",
      "\n",
      "New Tag : JJ\n",
      "[]\n",
      "Changing word:  ugly to awful\n",
      "\n",
      "New Tag : JJ\n",
      "[]\n",
      "Changing word:  ugly to horrible\n",
      "\n",
      "New Tag : JJ\n",
      "[]\n",
      "Changing word:  ugly to stupid\n",
      "\n",
      "New Tag : JJ\n",
      "[]\n",
      "Changing word:  ugly to scary\n",
      "\n",
      "New Tag : JJ\n",
      "[]\n",
      "Word being changed : ugly\n"
     ]
    }
   ],
   "source": [
    "def runThroughTweets():\n",
    "    numberOfTweets = 50;\n",
    "    tweets_df = pd.read_csv( pathToDatasets + 'cleanedTweets.csv' , nrows=1, skiprows=72 )\n",
    "    tweets = tweets_df.values\n",
    "\n",
    "    for counter,tweet in enumerate(tweets):\n",
    "        tweet = tweet[0]\n",
    "        mainSentiment = senty.polarity_scores(tweet)['compound']\n",
    "        if(mainSentiment == 0):\n",
    "            continue\n",
    "        print(\"\\n-NUM({2}) {0}:{1}-\\n\".format(tweet,mainSentiment, counter))\n",
    "        newStrings = getAlternativeSentences(tweet, mainSentiment)\n",
    "    \n",
    "    \n",
    "runThroughTweets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
